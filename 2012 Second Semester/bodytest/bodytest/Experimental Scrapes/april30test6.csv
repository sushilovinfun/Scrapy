topic_title,topic_url,post_author,post_body,thread_author
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">Hello booklovers,<br><br>My name is 123dutch.<br>I am from the Netherlands. And I am co-admin at uz-translations.net.<br>Our website specialises in everything to learn a language.<br>On uz-translations we are ideologists. We think that information should be free.<br>We would like everyone regardless of origin, religion, status and what more, to be able to gather the information they need.<br>We believe that to prevent books from dissapearing, they must be spread as much as possible.<br><br>We
 have a few limitations: we do not publish material on our website that 
IS already available for everyone at very low cost. Because we believe 
that if all publishers would be like that, we would not be needed 
anymore. Which would be a good thing.<br>Example: dutchgrammar.com is a 
dutch website where for something like 2 euro you can download 
everything from there to learn the dutch language - or <!-- m --><a class=""postlink"" href=""http://www.book2.de/"">http://www.book2.de/</a><!-- m --> where you can also for very little money download everything you need. <br><br><br>Recently
 I was able to download the complete LG through torrents. We installed 
it on our server using the same webinterface you are using. Our server 
is used for backup purpose of our website(s). So it only runs a 
localhost of LG.<br>So everything from 0 - 874000 is there now.<br><br>Then
 came our idea how to prevent all this to be lost in case of an 
emergency. Because harddisks can crash. Servers can be taken down. 
Torrents at LG can end. So how to make that extra backup in case of such
 a disaster?<br>Downloading all through torrents again would take too long and one would never know what can happen is such a long period.<br><br>In
 the Netherlands Usenet is very common to use. It has a few advantages 
over torrents - and also a few disadvantages. But for OUR goal it would 
fit perfectly.<br>The disadvantages: it is not possible to download 1 
single book - you can only download 1 book at a time / you need a payed 
usenetaccount to acces the files on Usenet.<br>The advantages: the files
 are stored on Usenet for 4.5 years before they are deleted / you can 
download from usenet at the speed of your internetconnection / uploading
 and downloading can be done anonymous.<br><br>Simultaneously we are 
reaching out to become more friends with communities that share the same
 ideology as we do. So we reached out to become friends with LG. And 
here we are now.<br><br>We know that there are many people that would like to download to entire collection. And from there-on spread further.<br>And
 this could also be a good way to have an EXTRA way for downloading 
everything, next to torrents. But as said earlier, you cannot just pick 1
 book - only 1000 at a time.<br>Downloading the entire 9TB collection 
over Usenet takes at most 2 weeks - depending on your internetconnection
 ofcourse. You can download to 100MB/s.<br><br>Uploading the entire LG to Usenet will take me about 3 months. End of march I hope to be ready.<br>Files are uploaded exactly as the contents of the torrents. With 1000 books. With the same foldernames. With the original MD5.<br>It
 is YOUR library so we will only add the files on our forum - no where 
else (only if you want us to) - and we will give the files to download 
ofcourse to you here at LG. If you wish to add them to your website or 
spread them further, I would only like to encourage that.<br><br><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">How it works</span></span><br>Let''s
 take folder 619000 per example. Everthing in the folder is rarred in 
pieces of 1GB. Added are par2 files (used on usenet to repair the rars).
 On usenet little things go wrong with uploading sometimes and the 
combination of RAR and PAR is used to be able to repair if neccesary and
 download in 100% exact state. The namegiving must always be unique - 
else finding things on usenet is difficult - so the generic namegiving I
 chose is: gen619000esis (and for 618000 this is ofcourse 
gen618000esis). <br>This gives then the following files: 
gen619000esis.part01.rar, gen619000esis.part02.rar etc PLUS 
gen619000esis.par2, gen619000esis.vol000+01.PAR2 etc. <br>If you search on usenet you will find all the files together when you search for gen619000esis<br>Let''s use binsearch.info to search gen619000esis:<br><!-- m --><a class=""postlink"" href=""http://binsearch.info/?q=gen619000esis&amp;max=100&amp;adv_age=1100&amp;server="">http://binsearch.info/?q=gen619000esis&amp; ... 00&amp;server=</a><!-- m --><br>You see here the entire 619000 collection: 9 par2 files, 12 rar, 1 NZB.<br><br>Currently the publishers have their arrows on Usenet search engines, like binsearch.info.<br>In
 the rare event that all usenet search engines would end, there is still
 the NZB file. The NZB file (like a .torrent) contains all the files  as
 you see when you would search for the files. Meaning that with 
downloading you will not need a search engine at all. Only the NZB file 
will be enough. You simply add the NZB into your newsreadingprogramm and
 the download starts automatically.<br><br>I am aware that downloading 
with Usenet is not so common in Russia. So should you have more 
questions feel free to ask. there is much more to explain if needed.<br><br>Here is more info how usenet works:<br><!-- m --><a class=""postlink"" href=""http://www.binaries4all.com/"">http://www.binaries4all.com/</a><!-- m --><br><br>And lastly: thank you very much for welcoming me into your home <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile""><br><br>Kind regards,<br>123dutch</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,are,"<div class=""postbody"">hi 123dutch,<br><br>this is very interesting! Thank you for spreading the books.<br><br>some
 people tried to download files from uz-translations, and one problem 
with uz-translations is that there is no way to download the entire 
collection of files. Is it available on usenet? Is there a list of all 
files, or some kind of database where all files are listed?</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">Dear are,<br><br>We are in the middle of a process in centralising the data.<br>At this moment  - other than one by one - it is not possible to download the entire collection.<br>After the proces this will be a little easier, but still dificult. <br>Many data on uz contains of books+audio. Should you want to download everything then we are talking about 50TB+ on data.<br>LG would probably be only interested in the books - not the audio. And then even only the books that are not on LG yet.<br><br>The centralising may take 4-6 months.<br>And
 we need some time to adjust to that idea as well. Sharing and spreading
 data IS eminent, but we are undergowing huge changes ourselves at the 
moment (all good) and after the changes we need to settle-in a little 
bit. Then we can think what would be the best way.<br>For the time being
 most of our data is already protected - I am saying """"most""""  because 
this is what the centralising of our data implies to begin with.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody"">from our <span style=""font-weight: bold""><a href=""http://genofond.org/viewtopic.php?p=25006#p25006"" class=""postlink"">previous discussion</a></span>
 only uz-trans whoud have maximally 300 GB of textual books (no audio 
etc), correcting for the excessive URLs in the uz-trans blog the number 
reduces to about 100 GB. It is harder to say how many intersection LG 
may have with uz-trans, but I expect it's a lot, more than 1000 books, 
which is a huge collection if to speak about a professional/dedicated 
topical library, we have at least one remarkable example collected by 
plantago on botany. Realistically we should expect probably <span style=""font-weight: bold"">5000-8000 new books</span>, because the number of our books on linguistics is probably about 2k. This is only from uz-trans.<br><br>welcome, and feel at home!</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,Bill_G,"<div class=""postbody"">повесил на главной объявления.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">Personally I expect more than 300GB.<br>In 
the process of getting all links centralised, LG is one of the main 
sites for us to check when files are missing to bring back to UZ, and 
sections as linguistic are often found here on LG, but specific language
 learning books often not.<br>My estimate would be more like 25000 books are missing on LG from UZ (not counting greylib, turklib).<br>The average post on uz has 2 books. We have 40.0000+ posts. Of these 40.000+ at least 50% is true language learning material.<br>This seems much, but it is what i see every day.<br><br>It does not matter so much.<br>I am thinking about a system how to get the books only, to LG.<br>I
 have no idea if one of you has a high speed internetconnection. Else 
(after the books are centralised) I could set up a torrent file to 1 
person here with all the needed books in. Something like that. Other 
ideas are welcome.<br>But - though I believe this must be done - many people on uz still need to get used to that idea. So give it a little time.<br><br>What I AM seeing is only positive changes at UZ, and a BIG increase on new visitors daily.<br>We
 have 380.000 registered members, and the number of active ones is 
unknown. I am guessing 10%. But as an example, 8 books of Cortina were 
published today (a must have for LG in my opinion by the way) and after 
10 minutes online we already counted 100 completed downloads. This gives
 me a little idea of how active the community is.<br><br>The 
transformation of the idea that books should spread is gradually 
spreading through other members. But for some people little steps are 
needed. Too big steps at once, and they will get scared with all the 
change. We admins LOVE the change by the way. We embrace it completely. 
And the path to take is obvious and inevitable to keep the books alive.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody"">uz-trans is a beautiful resource, it's 
obvious that people would wish to dwell there, unlike in LG. It is 
usually the case: if the project is liveable, it has a large permanent 
community. We observe the same effect with lib.rus.ec, flibusta.net, 
awaxhome etc etc. Humans need home, not just a beautiful engine.<br><br>On
 the side of the structure, I wish you regularize it in future for 
simpler handling, then centralization won't be a pain any longer. Of 
course when the project has been running for years with an unstructured 
accounting of the book records and files, it's gonna be quite an effort 
to first sort out the problems with the old records that need to be 
accurately collected etc. Since I've been trying to parse your site, I 
can say it is very hard and should mainly be done by hands. A very 
little effort in the beginning would've eliminated the need to heat the 
ocean at present. I mean the fields of a book record should've been 
fixed and the files should've been rigirously linked to the records. 
That's what we have in LG and, in fact, nothing else. The rest is we 
track that this link between the file and the record is not broken.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">It is true - we have had no structure at all
 for many many years - so everything IS done right now to manually get 
everything centralised.<br>Yes, this is much work. Hence the 4-6 months 
we expect to need for this. But after this things will be in control. 
Still much optimilisation will be needed then, but we will be ready to 
spread the neccesary material to LG.<br>But this is done by multiple hands - not only 1 anymore - all working for the same goal. <br><br>When
 it comes to further optimilisation after we centralised everything, 
this will be still an imense job. Since many of our posts need to have a
 interactive design because newer posts/better posts/completer posts to 
an existing post are added on a daily basis, it is more difficult to 
bring structure without losing the flexibility. For books only we could 
use isbn or something like that, but since in our case we are talking 
about multimedia material as well -and the for us needed coherency 
between the books and the added material - it needs a more complex 
design. <br>Currently we have no other option but categorising per language.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent"">For
 books only we could use isbn or something like that, but since in our 
case we are talking about multimedia material as well -and the for us 
needed coherency between the books and the added material - it needs a 
more complex design. <br>Currently we have no other option but categorising per language.</div>I'd
 split the categorization issues from the core part: how to store the 
record and the file - those two are the core, the rest should come 
along. I think it would help you a lot, if you prioritize the structural
 features: flexibility on the Web may stay exactly the same while the 
core information is collected in a rigorous/stable way. For example, if 
each book-thread has a field to enter URLs relevant to the book and a 
flag to show which records are out-of-date, you are done, your database 
will be tracking only the most recent information. It's sketchy, but the
 way of collecting links must be exceptionally robust. Even if you do 
not mark up the old records as obsolete, the collection of all links 
rigirously is already a big deal. In the most trivial way it may look as
 the HTML textarea. The posts may additionally be filtered for any 
presence of URLs to make the user fill in the textarea instead.<br><br>The
 thread head would needs some fixed set of field, as usual in 
bibliography. Users should not have unnecessary degrees of freedom at 
filling the records, it should all be templated. Otherwise they'll be 
entering anything they like in any format.<br><br>Though, it'll be 
difficult to make all fields rigorous, and absolute strictness will 
discourage many ot upload at all. Because bibliographic records are very
 complex in their general form. Many fields would need to be left for 
free style completion. For this reason in LG there are fields that are <span style=""font-weight: bold"">never</span>
 changed from the Web, they are only displayed in read-only mode: ID, 
MD5, filesize and file extension. Those are vital for the collection 
integrity and must not be changed. Everything else can be changed. I 
guess in your case you'd also need to distinguish what must be strict 
and what must not be, for the sake of flexibility.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">I agree that something or some things NEEDS to have a status that never change.<br>We
 will need to figure out how to do that. Because currently even POSTS 
itself need to be deleted quite regularly in order to have the most 
actual information present on the website.<br><br>If we were to make a 
system with parts that never change, then we really would need to start 
building from scratch. It can be done.<br>But this would take NOW more 
manpower than we can handle. So for the time being - till a major 
website upgrade is planned anyway - we have to at least gather all 
material centralised. Which could have been done more effectively if we 
had a system like you wrote out, true.<br><br>When UZ started in 2004 the needed information on every post was much different from now. No one expected to grow so much.<br>So
 over the years items were more and more difficult to find - we 
implemented some changes - but never from scratch. So in the website you
 see posts from 2004 which are already differenly structured that posts 
from 2008, and so on.<br><br>Thanks for you insights <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody"">from scratch maybe an overkill, just a 
replacement of the head message code in a book thread should suffice, 
one PHP-script, like in our case for the Librarian interface: <!-- m --><a class=""postlink"" href=""http://libgen.org/librarian/"">http://libgen.org/librarian/</a><!-- m --><br>(genesis:upload)<br><br>you can see the forbidden to modify entries as the uneditable fields below, for an example book <!-- m --><a class=""postlink"" href=""http://libgen.org/librarian/registration?md5=00b800ea9aee39af51a6c0d980c74398"">http://libgen.org/librarian/registratio ... d980c74398</a><!-- m --><br><br>if
 something functionally similar appears for each book, the rest of the 
site can be kept as is. The well known Open Library has a similar 
interface, when I first saw it, it was amazingly similar to our 
Librarian GUI. It's a good indication that other ways may miss something
 important. OL has invested a lot of efforts to a dedicated database 
engine to work with records, but the GUI hasn't gotten much more complex
 than ours (which is quite simple, but fills in a regular database 
structure).</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody"">after looking to the insides of uz-trans and
 seeing how the books get to the collection, it stroke me that it would 
be good to have a global book collector site with inified upload process
 that distributes books over the libraries registered in it (and 
supporting the protocol). Because it seems that everyone is fighting 
with the same set of technical problems, and many large libraries 
could've been much more efficient, if they had some protocols. I mean 
that releasing books in forum threads is a heavy approach: the book 
needs to be processed by many, manually. The post-moderation approach 
works fine too.<br><br>Many different specialized libraries is very 
good, but they need to follow some protocol, then humans can do what 
they do best, and the mighty ""computer"" (the global marshalling portal) 
will be doing the best it can: distributing books to different 
libraries, but keeping track of all of them in the database.<br><br>Of 
course, the database will be much-much more simple than that of 
OpenLibrary, e.g., but the number of electronically available books 
suggests that such a way is a lot more productive than making a perfect 
database without book files properly attached to it.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,ansaron,"<div class=""postbody"">@123dutch<br><br>Thank you very much for 
this! I am unable to use torrents as my service provider 
throttles/shapes all p2p traffic (I get 1 KB/s at best). This makes it 
entirely impractical to  download a multi-terabyte archive via torrents.
 Usenet, on the other hand, runs at full speed. You have made it 
possible for me to mirror libgen! (I have quite limited speed so this is
 still going to take 6-8 months -- better than 6-8 YEARS though.)<br><br>I do have one suggestion:<br>Would
 you consider increasing the percentage of PAR files? You are currently 
providing about 5% par2 recovery files. In one case (the gen10000esis 
fileset), using astraweb as my usenet provider, I was unable to recover 
from checksum mismatch errors -- 67 additional blocks were still 
required for repair. In this case I was able  to  successfully 
re-download this fileset from my ISP-provided newsserver (*very slow*) 
which gets its data from gigagnews. <br><br>It would be good to make the
 Usenet backup of libgen as robust as possible; increasing the 
percentage of available PAR2 files would go a long way to achieving 
this. While this would increase your upload times by 5-10%  it would 
greatly increase the chances of successfully downloading libgen material
 without having to use backup servers.<br><br>In any case, you're doing the world a great service. Thank you again!<br><br>@bookwarrior:
 Of course I also have to thank you and the libgen crew for creating 
this amazing library. I am sure thousands all over the world think kind 
thoughts for you and all your efforts.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent"">@123dutch<br><br>I do have one suggestion:<br>Would
 you consider increasing the percentage of PAR files? You are currently 
providing about 5% par2 recovery files. In one case (the gen10000esis 
fileset), using astraweb as my usenet provider, I was unable to recover 
from checksum mismatch errors -- 67 additional blocks were still 
required for repair. In this case I was able  to  successfully 
re-download this fileset from my ISP-provided newsserver (*very slow*) 
which gets its data from gigagnews. <br><br></div><br><br>Dear ansaron,<br><br>Thanks very much for your reply. It is nice to see that the NZB''s are actually used <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>Some newer NZB''s have been uploaded already, but I will increase par2 to 10%, starting with gen108000essis .<br><br>Kind regards,<br>123dutch</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,ansaron,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent"">Dear ansaron,<br><br>Thanks very much for your reply. It is nice to see that the NZB''s are actually used <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>Some newer NZB''s have been uploaded already, but I will increase par2 to 10%, starting with gen108000essis .<br><br>Kind regards,<br>123dutch</div><br><br>Dear 123dutch,<br><br>I'm
 sure there are many others out there making use of your Usenet uploads 
too -- and numbers will increase as word about the availability of the 
libgen archive via Usenet gets out. I do appreciate your decision to 
increaser the percentage of par2 repair files; Thanks!<br><br>I am now 
systematically downloading the available archives and came across one 
unusual error: In downloading the gen22000isis fileset  there were some 
checksum erros, but these were repairable by the parity files. However, 
the resulting repaired rar files were then reported as being corrupt. On
 OS X  the decompression tools refused to extract files, reporting 
either a corrupted  or passworded archive. I moved the rar files to a 
windows machine which extracted 998 out of 1000 files, reporting two 
files as  being corrupt:<br>!   I:\1358885413\gen22000esis.part01.rar: CRC failed in 22000\00b010f787b357e24133a8b976650e67. The file is corrupt<br>!   I:\1358885413\gen22000esis.part04.rar: CRC failed in 22000\650c08e11b676a4dc2f5ede8c8287a4c. The file is corrupt<br><br>I
 downloaded the archive twice; once using astraweb and once using 
TweakNews, so I don't think this is a Usenet provider problem (in any 
case the archive has been successfully repaired so the problem is 
deeper.) Could you check on this?<br><br>One additional thing: you're 
splitting up the rar files at the 1GB level; this is (based at least on 
my experience of Usenet) unusually large. Is there a reason for this? Is
 there slightly less overhead in uploading a smaller number of files? 
The reason I ask is that I'm getting more checksum errors than I'm used 
to seeing. Could it be that the very large size of the rar files is 
responsible? Would it be a burden to decrease the size of each rar file 
to, say, 100MB? (My usual Usenet provider astraweb is normally quite 
reliable -- except when they overeagerly obey DMCA takedown orders! --  
so it's a bit worrying.)<br><br>Please do let me know if you want these sort of reports, or if you'd prefer me to shut up  <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile"">.<br><br>In any case, with great gratitude,<br>ansaron</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">Haha, no please keep the remarks coming <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>1GB is big, but since some archives are 20G I thought we would get so many files... I can make the rar''s smaller if that helps.<br>I
 wonder if that changes the outcome. I am using giganews so I have no 
idea if that makes a difference towards other providers. 100MB on the 
other hand seems so small, since you would get really many files.<br><br>Now I am not verifying the rar. I will from now on.<br>Now I am not adding a recovery archive to the rar. Maybe I should.<br><br>So I would like to hear your opinion on number 2 and 3:<br>1.
 I raised the par2 to 10% already &lt;-- good for recovering, but the 
9TB size gets 5% bigger. (it is a choice to download all par2 files - so
 this is not a real problem - if you have giganews you will almost never
 need the par2 files)<br>2. I can add the recovery archive &lt;-- this 
would increase the chance of recovering (if used well), but also this 
would increase the total size of the archive. (this increases the total 
size for everyone)<br>3. making smaller sizes like 100MB parts instead of 1GB parts would really help?<br><br>kind regards,<br>123dutch</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,ansaron,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent""><br>So I would like to hear your opinion on number 2 and 3:<br>1.
 I raised the par2 to 10% already &lt;-- good for recovering, but the 
9TB size gets 5% bigger. (it is a choice to download all par2 files - so
 this is not a real problem - if you have giganews you will almost never
 need the par2 files)<br>2. I can add the recovery archive &lt;-- this 
would increase the chance of recovering (if used well), but also this 
would increase the total size of the archive. (this increases the total 
size for everyone)<br>3. making smaller sizes like 100MB parts instead of 1GB parts would really help?<br><br>kind regards,<br>123dutch</div><br><br>Good
 to know your original Usenet provider; as many of the checksum errors 
are introduced in the mirroring process Giganews might be a good choice 
for people intent on downloading libgen.  I was having a particularly 
bad time with astraweb. After I switched to TweakNews the number of 
corrupted files went down. I will certainly try GigaNews next.<br><br>In response to your points:<br>(1)
 Yes, it is a choice, and in fact some nzb clients are able to 
automatically decide if the par2 files are even required. Since some 
people will be using substandard ISP-provided nntp servers being 
generous with par files seems to be a very good idea. <br><br>(2) I 
think we should think in terms of maximizing the longterm robustness of 
 the Usenet backup of libgen. Since these files are hopefully going to 
be on Usenet servers for 4+ years (hopefully even longer as Usenet 
providers keep on extending their retention levels) we should aim at 
providing files that are are almost guaranteed to flawlessly decompress.
 Taking that longterm view means that an additional 5-10% downloading 
time is not a significant demerit. I vote for recovery archives.<br><br>(3)
 I honestly don't know. I have absolutely no experience of uploading to 
Usenet. My comments were based only on my extensive Usenet downloading. 
For now keep things as they are. I'm going to be using your NZBs 24/7 
from today. If I encounter too frequent errors I'll let you know and you
 can try and adjust the RAR  part size down.<br><br>A question:  do you have this set up as some sort of automated cron job or are you manually preparing these uploads?  <br><br>One
 last comment: you started posting the libgen archive with the ID 
CPP-gebruiker, then recently switched to CPP-user (I know enough Dutch 
to know that's the same thing <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile"">
 ).  The last libgen archive that you posted as CPP-gebruiker was 
gen105000esis which after 2 days is still incomplete, while later 
archives are complete. Is there possibly a problem with the 
gen105000esis upload?       <br><br>As always, my thanks and kindest regards,<br>A.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">Thanks for your comments again - they are welcomed.<br>I will make the adjustments for next uploads. <img src=""viewtopic.php_files/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>All
 is done manually. So I can simply make any adjustments without problem 
of things not uploaded yet. And if we encounter things that could have 
been uploader BETTER I will re-upload them.<br>ccp-user vs 
ccp-gebruiker: I was using my work-connection to upload since it much 
faster at that moment, and the upload killed my laptop uploading the 
gen105000eses. Killed the programm I use for uploading also 
(camelsystempowerpost). I tried some tricks - changing from the dutch to
 english version of the programm a few times - and it took me 4 days or 
so to finally find the problem. The user/gebruiker is from me using the 
english or dutch programm to upload. 105000 is still to be uploaded. I 
thought at some point that the error might be in the 105000, but it was 
not.<br>I uploaded the 106, 107 and 108 already. Will continue with the 105 next, using the new settings.<br><br>Please keep reporting so I have a list of things that need to be re-upped or not.<br>Thanks.<br><br>123dutch</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody"">123dutch<br><br>have you actually check 
MD5's before uploading to Usenet? It'll probably take a week, but it's 
worth doing, from our practice it always discovers surprizes. HDDs are 
imperfect, possibly 1 byte per billion isn't written correctly etc. Once
 the files settle in some machine it is desirable to do such a check 
once.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">123dutch<br><br>have
 you actually check MD5's before uploading to Usenet? It'll probably 
take a week, but it's worth doing, from our practice it always discovers
 surprizes. HDDs are imperfect, possibly 1 byte per billion isn't 
written correctly etc. Once the files settle in some machine it is 
desirable to do such a check once.</div><br><br>Good addition.<br>Texas is checking them now.<br>He started with 110.000 and up at my request now, before rarring them.<br>Those
 are all identical (except for a few files missing  - about 15 files or 
so are missing from the 9TB because they were infected (or most likely 
infected) by a virus - so IF you or anyone else sees an archive not 
containing 1000 but 998 books, then THAT is the reason).<br><br>So far 
all checks out  - the server we are using is quite fast, so checking md5
 will take about 6 minutes per 1000 (88hours) - all will be checked 
before uploading now. If we encounter an md5 error in the ones uploaded 
already, then I will re-up.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,ansaron,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">123dutch<br><br>have
 you actually check MD5's before uploading to Usenet? It'll probably 
take a week, but it's worth doing, from our practice it always discovers
 surprizes. HDDs are imperfect, possibly 1 byte per billion isn't 
written correctly etc. Once the files settle in some machine it is 
desirable to do such a check once.</div><br><br>@Bookwarrior: <br><br>It
 is really nice that you use the MD5 hash as the filename in libgen -- 
it makes the checking of a file's integrity self-contained.<br><br>If people are going to check the MD5s of libgen archives they might want to look at md5deep (<a href=""http://md5deep.sourceforge.net/"" class=""postlink"">http://md5deep.sourceforge.net/</a>). This has some very nice features compared to the usual md5sum program. The ones relevant to us are:<br><div class=""quotetitle""><b>Quote:</b></div><div class=""quotecontent"">1.
 Recursive operation - md5deep is able to recursive examine an entire 
directory tree. That is, compute the MD5 for every file in a directory 
and for every file in every subdirectory.<br>2. Comparison mode - 
md5deep can accept a list of known hashes and compare them to a set of 
input files. The program can display either those input files that match
 the list of known hashes or those that do not match.</div></div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody"">123dutch<br><br>somewhen after you filter 
out the suspecious infected files, we'd appreciate the listing: in most 
cases in the past it was false positive, but there are many executables 
in LG yet (man power problem). Although 1 in a 1000 looks quite wrong, 
especially if it's in the first 100k. But it's all possible of course.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,ansaron,"<div class=""postbody""><span style=""font-weight: bold"">Report on Libgen Usenet Postings gen0esis--gen25000esis:</span><br><br><span style=""font-weight: bold"">Directory:.......# Files:.......MD5 Mismatches:</span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">    /0        999       0      <br> /1000       1000       0<br> /2000       1000       0<br> /3000       1000       0  <br> /4000       1000       0<br> /5000       1000       0 <br> /6000       1000       0<br> /7000       1000       0<br> /8000       1000       0<br> /9000       1000       0<br>/10000       1000       0<br>/11000       1000       0<br>/12000       1000       0<br>/13000       1000       0<br>/14000       1000       0<br>/15000       1000       0<br>/16000       1000       0<br>/17000       1000       0<br>/18000       1000       0<br>/19000       1000       0<br>/20000       1000       0<br>/21000       1000       0<br>/22000        998       1 (Repost: 1000 files, 0 MD5 Mismatches)<br>/23000       1000       0<br>/24000       1000       0<br>/25000       1000       0<br></div><br>All archives except gen22000esis expanded correctly. <br><br>In
 gen22000esis the rar archive was reported as being corrupted. 998/1000 
files were extracted, with the remaining two being reported as corrupt. 
In addition md5deep found a hash mismatch for the file 
22000/f54a3bce73a27a235960db70c636aded  <br><br>Conclusion: gen22000esis needs to be re-upped.<br><br>I will post reports on 123dutch's Usenet uploads (including MD5 verification) in batches of 25000.<br><br><span style=""font-weight: bold"">EDIT:</span> gen22000esis-repost.nzb is perfect. It decompresses without error and there are no MD5 mismatches.</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,bookwarrior,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent"">I will post reports on 123dutch's Usenet uploads (including MD5 verification) in batches of 25000.</div>perfect!!! Thanks a lot!</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/viewtopic.php.html,123dutch,"<div class=""postbody"">The 22000 is one of the archives before I verified rar''s... I downloaded this one, extracted, and it gives 2 crc errors.<br>The 22000 will be reupped today, name: gen22000esis-repost  , but ofcourse the nzb will be added to the repository as well.<br><br>Texas
 is busy verifying all archives for missing books. Also for MD5 
mismatches. He will probably finish today or tomorrow and report his 
findings here.<br><br>Note: today I encountered the first time with 
creating par2 files that quickpar gave an error creating the files. I 
did my usually tests and tricks  - and sometimes changing the settings 
in quickpar could fix it - but the bigger the archive the more difficult
 to fix. On the net is explained that when you have too much internal 
memory quickpar can give these kind of errors. The only one that I could
 not make par files for was 121000 so far (which is a huge archive of 
33GB) - not even with another programm. But on my laptop I have only 4GB
 so I am making it there with no problem (just takes very long)</div>",123dutch
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,bookwarrior,"<div class=""postbody"">i guess you can just split it into two parts, why to keep the 1-to-1 identity with LG? there is no reason for this. I.e. you can make two files<br>gen121000esis-part1of2<br>gen121000esis-part2of2<br><br>that's pretty much it, if I understood the problem. No animals will be harmed.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,texas,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">123dutch<br><br>somewhen after you filter out the suspecious infected files, we'd appreciate the listing: in most cases in the past it was false positive, but there are many executables in LG yet (man power problem). Although 1 in a 1000 looks quite wrong, especially if it's in the first 100k. But it's all possible of course.</div><br><br>Hi bw, we finished checking all the folders regarding the number of files in each one, and a virus-check on them.<br><br>We found only 3 virus detected by MS Essentials (all .exe files):<br>118000\fdc00245581b73b84d50cb4ef7a6143d<br>210000\61415ac7cfb96b7fcb967f5532b073e9<br>224000\140d912e60dd2fd88f75b9d7f2c6a653<br><br>In the past, I had some experiences with these ebook compilers that were identified as virus, though there were false positives, and maybe it's the case (better recheck with other antivirus as well).<br><br>About the MD5 checks (thanks ansaron and bw for the idea &amp; checks), we'e already on the folder 250.000, and in next days we hope to finish this as well. In the files already uploaded in Usenet, we found only 5 files with MD5 errors, which are:<br>35000\342ee41dd4c552bb8a14885b3064dbcb<br>35000\4f03f64700ca520a36ce3d7ea6356dfc<br>37000\e950c65a6e4e4df7e3a6d7a5cbe72e13<br>46000\6ef53368dd0404612fc4dbe932dbe21a<br>49000\211d062cd4a5bf38380bb413446df433<br><br>We are thinking if we reupload all these folders again, with the correct files fixed, or just these 5 files to be replaced (as a 'patch', so people download and overwrite our uploaded folders with them). Now we are double checking to avoid any MD5 or unrarring errors in future NZBs  <img src=""./Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet_files/icon_e_smile.gif"" alt="":)"" title=""Smile""><br><br>Please keep coming with suggestions, ideas and checking the files, all this helped us to fix a lot of things too <img src=""./Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet_files/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,bookwarrior,"<div class=""postbody"">yes, that's how it's been happening with us: first upload of LG of about 50k took me 3 almost full attempts, making me iteratively learn what the issues were, just think of this number, it's satanic, if to have no mechanism of file integrity checking. If batches of folders arrive with no errors, it is more a pleasant exception, than a rule. But once you check them after writing onto an HDD, they are going to be more stable, than during the transfer process (including HDD writing). Reading isn't harmful, the rest of the file lifetime will only depend on the hardware lifetime which is the fundamental limit, from our statistics no longer that 1 year per HDD in average when files are accessed randomly, but I can't give a figure how often, possibly at maximum ~10 files/min in average.<br><br>btw, it's not only about HDDs of course, TCP/IP also admits some rate of errors, it does not absolutely guarantee delivery, it has checking mechanisms, but they do not guarantee all 100%.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody"">Dear 123dutch,<br><br>I've downloaded the repost of gen22000esis.<br><br>First, the change to 100MB rar segments has helped: this is the first large (&gt;5GB) libgen archive that I've downloaded that did not require any par files. It also decompressed without error.<br><br>All MD5 hashes are verified too. Outstanding!<br><br>I'll edit my previous report post to point out that the repost of gen22000esis is error free. <br><br>Thanks,<br>A.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,Guest,"<div class=""postbody"">Hi 123dutch,<br><br><!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_nzb/gen107000esis.rar"">ftp://libgen.org/repository_nzb/gen107000esis.rar</a><!-- m --> is broken. If I search for the nzb elsewhere it says that it's incomplete: <!-- m --><a class=""postlink"" href=""http://www.nzbsearch.net/search.aspx?q=gen107000esis&amp;st=5"">http://www.nzbsearch.net/search.aspx?q= ... 0esis&amp;st=5</a><!-- m --><br><br>Can you please repost it? Thanks.<br><br>Thank you</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen26000esis–gen50000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br>/26000        1000        0 <br>/27000        1000        0<br>/28000        1000        0<br>/29000        1000        0<br>/30000        1000        0<br>/31000        1000        0<br>/32000        1000        0<br>/33000        1000        0<br>/34000        1000        0<br>/35000        1000        2 (0 with patch)<br>/36000        1000        0<br>/37000        1000        1 (0 with patch)<br>/38000        1000        0<br>/39000        1000        0<br>/40000        1000        0<br>/41000        1000        0<br>/42000        1000        0<br>/43000         999        0<br>/44000        1000        0      <br>/45000        1000        0<br>/46000        1000        1 (0 with patch)<br>/47000         999        0        <br>/48000        1000        0 <br>/49000        1000        1 (0 with patch)       <br>/50000        1000        0<br></div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>All archives except gen43000esis and gen47000esis expanded correctly.  <br><br>In gen43000esis the rar archive was reported as corrupt. 999/1000 files were extracted with the remaining file being reported as corrupt:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">!   F:\gen43000esis\gen43000esis.part3.rar: CRC failed in 43000\6e7c0c60951c7cc6a06ff67dfc6907ec. The file is corrupt<br></div><br>In gen47000esis the rar archive was reported as corrupt. 999/1000 files were extracted with the remaining file being reported as corrupt:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">!   F:\gen47000esis\gen47000esis.part4.rar: CRC failed in 47000\af253377df19e1631cc37ae2600ad74d. The file is corrupt<br></div><br>In both cases my OS X unrar utilities were unable to decompress the rar archives; WinRar was however able to extract all of the uncorrupted files.<br>All of the files that were extracted from these two archives were verified as having the correct MD5 hash.<br><br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>In addition md5deep found MD5 hash mismatches for the files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 does NOT match<br>/35000/8be66d2107a296a63c2f526df184f37d does NOT match<br>/37000/e950c65a6e4e4df7e3a6d7a5cbe72e13 does NOT match<br>/46000/6ef53368dd0404612fc4dbe932dbe21a does NOT match<br>/49000/211d062cd4a5bf38380bb413446df433 does NOT match<br></div><br>(texas and 123dutch already know about the problems with three of these five files. Their MD5 mismatch results for /35000 do not correlate with mine. Please check.)<br><br><br><span style=""font-weight: bold"">Suggestions: <br></span>(1) Create a `patch' file to replace the files with MD5 hash mismatches (re-uploading whole archives is overkill).<br>(2) Upload corrected versions of gen43000esis and gen47000esis since the rar archives are rejected entirely by many decompression utilities.<br><br><br>As always, thanks to 123dutch and texas for all their work in getting Library Genesis onto Usenet.<br><br><span style=""font-weight: bold"">Edit:</span> <br>(1) Remark on MD5 mismatches in /35000 files.<br>(2) (9 February 2013): gen PATCH esis corrects the mismatch errors in /35000, /37000, 46000, 49000. In the case of /35000 you should also delete the existing two files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 <br>/35000/8be66d2107a296a63c2f526df184f37d</div></div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,texas,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent""><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>In addition md5deep found MD5 hash mismatches for the files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 does NOT match<br>/35000/8be66d2107a296a63c2f526df184f37d does NOT match<br>/37000/e950c65a6e4e4df7e3a6d7a5cbe72e13 does NOT match<br>/46000/6ef53368dd0404612fc4dbe932dbe21a does NOT match<br>/49000/211d062cd4a5bf38380bb413446df433 does NOT match<br></div><br>(texas and 123dutch already know about the problems with three of these five files. Their MD5 mismatch results for /35000 do not correlate with mine. Please check.)<br><br></div><br><br>Thanks again, ansaron, for your great work checking the files, about the corrupted RAR Archives, I believe dutch will reupload them soon. <br><br>Regarding the MD5, the best way indeed will be to upload just a patch with the 5 files, however, regarding the /35000 folder (with the 2 problematic files), I believe they were replaced in the next versions of the LG database. <br><br>For example, the file 4e04cd97600452a602cae12f28ab43c1 isn't found in libgen.org if we perform a MD5 search. But, if we hash the actual file, the MD5 will be 4f03f64700ca520a36ce3d7ea6356dfc (so there's a mismatch as you correctly observed), and now this file is in LG:<br><br><!-- m --><a class=""postlink"" href=""http://libgen.org/book/index.php?md5=4F03F64700CA520A36CE3D7EA6356DFC"">http://libgen.org/book/index.php?md5=4F ... 7EA6356DFC</a><!-- m --><br><br>The same with the 8be66d2107a296a63c2f526df184f37d, whose MD5 is 342ee41dd4c552bb8a14885b3064dbcb, and we find <!-- m --><a class=""postlink"" href=""http://libgen.org/book/index.php?md5=342ee41dd4c552bb8a14885b3064dbcb"">http://libgen.org/book/index.php?md5=34 ... 5b3064dbcb</a><!-- m --><br><br>So, I believe that when the torrent was started, these 2 files had errors and only after, the database was fixed, but the torrents kept the original files (please bw and Bill, correct me if I'm wrong). <br><br>The other 3 files indeed had erros in our folders. I uploaded the patch <a href=""http://www.filefactory.com/file/2m9mhyaptq0p/n/LG_Patch_rar"" class=""postlink"">here</a>, but soon dutch will put this rar in Usenet.<br><br>Thanks again, and please keep bringing these infos, and any suggestions.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody"">texas, thank you very much for the reply, which is very reassuring. I was puzzled and quite concerned by this anomaly. I am happy though that the checks we have in place are picking up these problems –- the underlying LibGen filing system is well thought out and is doing its job.<br><br>The patch file seems perfect. There are now no MD5 mismatch errors in the corrected directories. Once the patch file is uploaded to Usenet I will edit the previous report, including a note that the two anomalous files in /35000 need to be deleted.<br><br>The error rate is very low. As bookwarrior pointed out we should expect some errors in the early stages. I think once we get past /104000 the combination of MD5 checks + RAR repair files + smaller RAR segments will force the error rate down to zero (I hope!). <br><br>Best wishes,<br>A.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,bookwarrior,"<div class=""postbody"">I believe there must be some optimal length of the crc code for a particular error correction/recovery algorithm. Possibly RAR has some open numbers or some easy tests can be made with the archiver itself (pack with and without recovery option, compare the sizes) to figure out what fraction of the data the recovery code should optimally take. With a binary/hex editor one can modify 1, 2, ..., N bits to see how the algorithm reacts. The numbers on the reliability of HDD writing and TCP/IP can be found, this will give a firm figure (I someone is heroic enough to get thru) what the reasonable amount of recovery information should be per unit of volume. I'm just speculating...</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,silverware,"<div class=""postbody"">Re: error rates - see this W#kipedia article <a href=""http://en.wikipedia.org/wiki/ZFS#Error_Rates_in_Harddisks"" class=""postlink"">http://en.wikipedia.org/wiki/ZFS#Error_Rates_in_Harddisks</a></div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,reopert,"<div class=""postbody"">there are several broken nzb files on <!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_nzb/"">ftp://libgen.org/repository_nzb/</a><!-- m --> nzb's 118-121 that is - most files come out broken. If I get the nzb from another source like <!-- m --><a class=""postlink"" href=""http://www.nzbsearch.net/search.aspx?q=gen121000esis&amp;st=5"">http://www.nzbsearch.net/search.aspx?q= ... 0esis&amp;st=5</a><!-- m --> it works fine. Thank you.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,123dutch,"<div class=""postbody"">this tuesday the upload continues again: 43000, 47000, 107000 and contuing with 131000 and up.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen51000esis–gen75000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br>/51000        1000        0 <br>/52000        1000        0<br>/53000        1015?       0<br>/54000        1000        0<br>/55000        1001?       0<br>/56000        1000        0<br>/57000         999        0<br>/58000        1000        0<br>/59000        1000        0<br>/60000        1000        0<br>/61000        1000        0<br>/62000        1000        0<br>/63000        1000        0<br>/64000        1000        0<br>/65000        1000        0<br>/66000        1000        0<br>/67000        1000        0<br>/68000         999        0<br>/69000        1000        0     <br>/70000         997        0<br>/71000         996        0<br>/72000         998        0       <br>/73000         997        0<br>/74000        1000        0        <br>/75000         994        0<br></div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>NO CORRUPTED ARCHIVES. All archives expanded correctly.  <br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found NO MD5 MISMATCHES.<br><br><span style=""font-weight: bold"">File Numbers:<br></span>Could I have confirmation of the correct/expected number of files in each directory? <br><br><span style=""font-weight: bold"">Remark on Patches:<br></span>gen PATCH esis corrects previous MD5 mismatches in /35000, /37000/, /46000 and /49000. A note about this will be added to the previous report.<br><br>As always, thanks to 123dutch and texas for their work. Your efforts are deeply appreciated. The error rate for this batch was zero. Outstanding!</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,reopert,"<div class=""postbody"">any reason why 81000 and 82000 are missing? they were skipped and went right into 83000</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody""><div class=""quotetitle"">reopert wrote:</div><div class=""quotecontent"">any reason why 81000 and 82000 are missing? they were skipped and went right into 83000</div><br><br>They do not exist/are empty.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,texas,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent""><br><span style=""font-weight: bold"">File Numbers:<br></span>Could I have confirmation of the correct/expected number of files in each directory? <br></div><br>Here is the list of the number of files in each directory, according to the torrent files. In our NZBs we just removed 3 files, detected as virus by MS Essentials (.exe files), so these 3 folders will have 999 files in the NZBs. The list is:<br><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">#Files  Folder<br>----   -----<br>54    /83000<br>257   /77000<br>290   /80000<br>318   /865000<br>351   /866000<br>515   /76000<br>913   /212000<br>917   /213000<br>985   /52000<br>994   /75000<br>996   /200000<br>996   /237000<br>996   /71000<br>997   /107000<br>997   /248000<br>997   /308000<br>997   /70000<br>997   /73000<br>997   /869000<br>998   /108000<br>998   /124000<br>998   /201000<br>998   /202000<br>998   /225000<br>998   /72000<br>998   /721000<br>998   /844000<br>999   /0<br>999   /104000<br>999   /114000<br>999   /115000<br>999   /118000 * virus (.exe - /118000/fdc00245581b73b84d50cb4ef7a6143d)<br>999   /203000<br>999   /210000 * virus (.exe - /210000/61415ac7cfb96b7fcb967f5532b073e9)<br>999   /223000<br>999   /224000 * virus (.exe - /224000/140d912e60dd2fd88f75b9d7f2c6a653)<br>999   /226000<br>999   /239000<br>999   /244000<br>999   /251000<br>999   /294000<br>999   /307000<br>999   /327000<br>999   /57000<br>999   /786000<br>999   /867000<br>1001   /55000<br>1015   /53000<br><br>All other folders = 1000 files</div><br><br>Some folders indeed have much less than 1000 files. Maybe BW or Bill can confirm or correct these numbers, if possible, so our NZBs can be up-to-date or eventually corrected to the actual LG now.<br><br>Thanks again for checking the NZBs! Your help and of course BW and Bill_G efforts are also outstanding, no doubt at all!  <img src=""./Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet_files/icon_e_biggrin.gif"" alt="":D"" title=""Very Happy""><br><br>Cheers!<br>Texas</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen76000esis–gen100000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br> /76000          515        0 <br> /77000          257        0<br> /78000         1000        0<br> /79000         1000        0<br> /80000          290        0<br> /81000         DOES NOT EXIST<br> /82000         DOES NOT EXIST<br> /83000           54        0<br> /84000         1000        0<br> /85000         1000        0<br> /86000         1000        0<br> /87000         1000        0<br> /88000         1000        0<br> /89000         1000        0<br> /90000         1000        0<br> /91000         1000        0<br> /92000         1000        0<br> /93000         1000        0<br> /94000         1000        0     <br> /95000         1000        0<br> /96000         1000        0<br> /97000         1000        0       <br> /98000         1000        0<br> /99000         1000        0        <br>/100000         1000        0</div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>There were NO CORRUPTED ARCHIVES. The gen78000esis archive <span style=""font-style: italic"">was</span> reported as being corrupt by MacPAR deluxe's built-in RAR extractor. However all 1000 files were successfully extracted by other RAR utilities (on both OS X and Windows). WinRAR confirms all files in the archive as being uncorrupted. All MD5 hashes for extracted files are OK. I have no explanation for this (beyond that most OS X decompression utilities seem more fragile than their Windows counterparts). So, if you have trouble expanding the gen78000esis RAR archives try a different decompression utility.    <br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found NO MD5 MISMATCHES.<br><br><br>The error rate for this batch was again zero. We are winning  <img src=""./Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet_files/icon_cool.gif"" alt=""8-)"" title=""Cool""></div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,reopert,"<div class=""postbody"">gen155000esis is missing at least one .rar file. It ends with part120, but it's incomplete - at least part 121 is missing. Getting only 992 files.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,123dutch,"<div class=""postbody"">I will re-up gen155000esis monday <img src=""./Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet_files/icon_e_smile.gif"" alt="":)"" title=""Smile""> Thanks for the reply <img src=""./Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet_files/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,bookwarrior,"<div class=""postbody"">our gen.lib.rus.ec's hoster seems to have sold his project (lib.rus.ec) or is close to this. No one knows what the fate of gen.lib.rus.ec is. So, the backups of any sort become even more important now.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,123dutch,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">our gen.lib.rus.ec's hoster seems to have sold his project (lib.rus.ec) or is close to this. No one knows what the fate of gen.lib.rus.ec is. So, the backups of any sort become even more important now.</div><br><br>LG on usenet will continue (I am sick for a few weeks, hence the somewhat slower uploads). <br>But no matter what, we will keep on pushing the files to Usenet.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody"">@123dutch<br><br>Someone else pointed this out before, but it's only just bitten me: gen118000esis -- gen121000esis are all incomplete (no matter which  NZBs you use). That is: the uploads are all missing too many blocks for repair to be possible (in many cases the PAR files are damaged too). <br><br>Would it be possible for you to either upload additional PAR files or else maybe totally re-upload these four archives again? At the moment they are damaged beyond repair.<br><br>Many thanks (and I hope you get better soon!).<br><br><span style=""font-weight: bold"">UPDATE:</span> I'm now trying to use the recovery volumes in the RAR volumes to first (partially) repair the RAR archives and then use the PAR files to complete the recovery of the archives. I seem to have succeeded in doing this for gen118000esis. It's taking some time so my next report will probably be delayed.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,ansaron,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">How to use RAR Recovery Records. Part 1<br></span></span><br>From gen105000esis onwards 123dutch has incorporated recovery records into the RAR archives he is posting on Usenet. This was new to me, so I thought I'd write a short How-To on using RAR recovery records. <br><br>I'll illustrate using two examples from 123dutch's Usenet upload of Library Genesis: gen126000esis and gen118000esis<br><br>We'll start with gen126000esis as this is more straightforward. After downloading the RAR volumes and repairing with PAR2 files, the standard decompression utilities reported that the archive was damaged. <br><br><span style=""font-weight: bold"">Windows</span><br>If you are using WinRAR on a Windows machine then open any of the gen12600esis RAR volumes (say gen126000esis.part001.rar) in WinRAR and select ""test"". <br>After a while WinRAR will identify one volume (gen126000esis.part005.rar) as containing a file with a CRC error.<br>Open gen126000esis.part005.rar with WinRAR and select ""repair"". <br>After a while WinRAR will produce a file: fixed.gen126000esis.part005.rar. <br>Delete the original file gen126000esis.part005.rar and rename fixed.gen126000esis.part005.rar to gen126000esis.part005.rar<br><br>gen126000esis should now decompress without any problems.<br><br><span style=""font-weight: bold"">*nix<br></span>If you are using a *nix variant (linux, OS X, ...) then you will need to use the command-line version of RAR (available from:<a href=""http://www.rarlab.com/download.htm"" class=""postlink"">http://www.rarlab.com/download.htm</a>).<br><br>We start by testing the RAR volumes to identify the volume(s) that requires repair. Change to the directory containing the rar executable and in a terminal window type:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">./rar t '/path/to/rar/volumes/*'</div><br>You should get output that reads:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">RAR 4.20   Copyright (c) 1993-2012 Alexander Roshal   9 Jun 2012<br>Trial version             Type RAR -? for help<br><br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part001.rar<br><br>Testing     126000/00528e2366972d6c027fb4a15e4b18cf                   OK <br>Testing     126000/0093fe6589d2a286206f9d181f9b6d4b                   OK <br>Testing     126000/00c1b48fda5f2f0072710c19a6f2a4c1                   OK <br>Testing     126000/00e61e1d50e9f106cca8e37d093fcaed                   OK<br>...<br>...<br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part005.rar<br><br>...         126000/0792160377d05abb306e2bac7cc43c5f                   OK <br>Testing     126000/079e0eb1cdfbc062e75758ccef635a61                    2%<br>126000/079e0eb1cdfbc062e75758ccef635a61 - CRC failed<br>Testing     126000/07d10822167b765b0031b5975b6a1ea2                   OK <br>Testing     126000/07dbc84267f12801736dbafcf791661f                   OK <br>Testing     126000/0807b545d7f25c85cc9dc0068ec816be                   OK <br>Testing     126000/08a244119244b8efe3c6f04b45ab5527                   OK <br>Testing     126000/08be1615b9af3c644a222012e775d9e0                   OK <br>Testing     126000/0910705a9d46846f5efdcac1e1d1b146                   OK <br>Testing     126000/092a34a90c230bc814dd3163398ca4e7                    2%<br>...<br>...<br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part086.rar<br><br>...         126000/fb931f2d8b7c464ed78c6f1e51cca6a7                   OK <br>Testing     126000/fbb9b0426e09282efcb2164f04a253dc                   OK <br>Testing     126000/fbdbcbe36bb3e00e805ee3f9470be247                   OK <br>Testing     126000/fc2d16b77ef81815aaf9aee3a154ced8                   OK <br>...<br>...<br>Total errors: 1<br></div><br>So in this case there is a single RAR volume (part005) containing a corrupted file.  We now repair that RAR volume:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">./rar r /path/to/rar/volumes/gen126000esis.part005.rar</div><br>You should get output:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">RAR 4.20   Copyright (c) 1993-2012 Alexander Roshal   9 Jun 2012<br>Trial version             Type RAR -? for help<br><br>Building fixed.gen126000esis.part005.rar<br>Scanning...<br>Data recovery record found 100%<br>Sector 36051 (offsets 119A600...119A800) damaged - data recovered<br>Sector 36052 (offsets 119A800...119AA00) damaged - data recovered<br>Sector 36053 (offsets 119AA00...119AC00) damaged - data recovered<br>Sector 36054 (offsets 119AC00...119AE00) damaged - data recovered<br>Sector 36055 (offsets 119AE00...119B000) damaged - data recovered<br>Done</div><br>and you end up with a file fixed.gen126000esis.part005.rar (in the same directory where the rar executable lives). Replace the original part005 volume with the fixed volume.  The  gen126000esis archive should now decompress without error.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,bookwarrior,"<div class=""postbody"">thanks, I believe it will be helpful to others.</div>",bookwarrior
Library Genesis on Usenet,file://localhost/Users/maxwellfoxman/Downloads/Library%20Genesis%20%20Miner's%20Hut%20%20%20%D0%91%D0%B0%D1%80%D0%B0%D0%BA%20%D1%81%D1%82%D0%B0%D1%80%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B8%CC%86%20%E2%80%A2%20View%20topic%20-%20Library%20Genesis%20on%20Usenet.html,texas,"<div class=""postbody"">@ansaron: Thanks a lot for your tutorial, and for you testings as well!<br><br>@BW: Today and in next days were uploading the books from folders 200k-300k, and we noticed that there's the fllowing batch file which deletes some books with broken links (MD5 mismatches?):<br><!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_torrent/del%20broken%20files.bat"">ftp://libgen.org/repository_torrent/del ... 0files.bat</a><!-- m --><br><br>So, it's better to delete the books from these folders, by running the .bat, and after they are removed, upload in Usenet the folders without these books already?<br><br>Thanks you all again for your great work!</div>",bookwarrior
