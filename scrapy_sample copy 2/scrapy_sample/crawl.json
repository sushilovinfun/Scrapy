[[[[{"content": ["We have decided to shutdown the Scrapy Snippets site.", "The snipets hosted there have been imported into Snipplr and can be accesed here:"], "link": ["http://blog.scrapy.org/scrapy-snippets-shutdown"], "title": ["Scrapy Snippets shutdown"]},
{"content": ["The main reasons for dropping support for Python 2.5 are:", "The code cleanup and the changes haven't been done yet, nor they will happen all at once in a single commit, so Scrapy 0.15 still works on Python 2.5. Not for long however, since we will be making these changes during the remaining development phase of 0.15."], "link": ["http://blog.scrapy.org/scrapy-dropping-support-for-python-25"], "title": ["Scrapy will drop support for Python 2.5"]},
{"content": ["After 10 months of work, and many changes, we are pleased to announce the release of Scrapy 0.14. "], "link": ["http://blog.scrapy.org/scrapy-014-released"], "title": ["Scrapy 0.14 released"]},
{"content": ["Scrapy users have complained in the past about the lack of a pre-built example project that contains, for example, the dmoz spider described in the tutorial.", "Complain no more!. We're happy to let you know that there is a now functional Scrapy project available on Github which contains the old Google Directory spider and the Dmoz spider described in the tutorial.", "The project is called \"dirbot\", and it's available at\u00a0", "\u00a0", "The documentation of Scrapy 0.13 (which will become the next stable release, Scrapy 0.14) has been updated to point to this new example project."], "link": ["http://blog.scrapy.org/new-example-scrapy-project-available"], "title": ["New example Scrapy project available"]},
{"content": ["Hi everyone,", "In an effort to make Scrapy code smaller and more reusable, we\u2019ve been working\non splitting the Scrapy codebase into two different modules:", "A library with simple, reusable functions for working with URLs, HTML, forms,\nand HTTP. Things that aren\u2019t found in the Python standard library. This library\ndoesn\u2019t have any external dependency.", "For more info see:\n* ", "\n* ", "Scrapely is library for extracting structured data from HTML pages. What makes\nit different from other Python web scraping libraries is that it doesn\u2019t depend\non lxml or libxml2. Instead, it uses an internal pure-python parser, which can\naccept poorly formed HTML. The HTML is converted into an array of token ids,\nwhich is used for matching the items to be extracted.", "Scrapely depends on numpy (it uses it to speed up calculations) and w3lib.", "You can find more info, or try it out, in the Github page.", "After these changes, Scrapy codebase has been reduced by 4574 lines, including\nblank and comments (according to ", ").", "Before:", "After:", "Scrapy 0.14 will depend on w3lib. Scrapy 0.13 (current dev version) already\ndepends on w3lib, but w3lib is already packaged and provided in the official\nAPT repos (package python-w3lib). So, if you\u2019re using Scrapy 0.13 on Ubuntu,\nyou can upgrade safely. Otherwise, you can always install/upgrade with\n", " or ", ". Stable version (Scrapy 0.12) is not affected at all by\nthis change.", "If you have any comments or questions feel free to post them in the ", " group."], "link": ["http://blog.scrapy.org/introducing-w3lib-and-scrapely"], "title": ["Introducing: w3lib and scrapely"]},
{"content": ["Hello everyone, we\u2019re pleased to announce the release of Scrapy 0.12!", "This release is based on the last development branch, aka. Scrapy 0.11.", "Starting from this release, we\u2019ll be following the ", ".\nThat means trunk is now Scrapy 0.13 and will became Scrapy 0.14 on next\nrelease.", "Notable changes of this release:", "For a more detailed list of changes, check the ", "."], "link": ["http://blog.scrapy.org/scrapy-012-released"], "title": ["Scrapy 0.12 released"]},
{"content": ["It is well known that many websites show different content depending on the\nregion where they\u2019re accessed. For example, some retailer sites show products\navailable only for the region (US, Europe) of the user accessing the site.", "Although this can be quite convenient for the website customers,\nit can be a pain for developers writing a spider for the site and running it from\ntheir local machines.", "There is a simple way to proxy all requests as if they came from another\nserver. You only need SSH access to this other server, no need to install\nany HTTP proxy. For this, you can use a program called\n", ".", "Here\u2019s how to do it in Ubuntu, though this recipe should be easy to extended to other\nLinux distros.", "First, install tsocks with:", "Then add this content to ", " (this file may vary across distributions):", "Next, SSH to the remote server you want to use:", "And finally, in another terminal (without closing the SSH console), just run Scrapy by prefixing it with the ", " command, like this:", "That\u2019s all. Your spider will run in your local machine but proxying all\ncommunication through the remote server. No need to change any settings or configuration."], "link": ["http://blog.scrapy.org/spoofing-your-scrapy-bot-ip-using-tsocks"], "title": ["Spoofing your Scrapy bot IP using tsocks"]},
{"content": ["A common question in the ", " is how to scrape AJAX sites. As Mark Ellul\u00a0", "\u00a0in the ", " mailing list, there are two basic types of AJAX requests\u00a0that web sites make use of. These are: \"static\" requests which their parameters\u00a0(URL, post data) doesn't change, and \"dynamic\" requests that use some variables\u00a0based on properties from the current page.\u00a0", "The general approach when dealing with \"static\" AJAX requests is adding their\u00a0URLs to ", "attribute as a \"normal\" URL. And to deal with \"dynamic\"\u00a0ones we will try to generate the same requests from Scrapy.", "To help us in this task, we'll use a Firefox add-on called\u00a0", ". This add-on comes with a\u00a0", "\u00a0that let us monitor the requests\u00a0being sent to the server and their responses", "We will scrape\u00a0", ".\u00a0When loading the site we can see that the page loads the gallery information\u00a0from another source, so to find it out we launch Firebug, go to the Net panel\u00a0and reload the page.", "In the Net panel, we see each request (and its response) made to load the entire\u00a0page contents, here we can filter the requests and look for ", " in\u00a0the XHR tab.\u00a0", "In the XHR tab, we see that two requests are made, one to\u00a0", "\u00a0and one\u00a0to\u00a0", ". If we look at the response of the first one\u00a0(clicking on it and then going to response tab) we see that it holds the gallery\u00a0slider data.", "Now, if we navigate to another photography, clicking on its slider link we'll\u00a0see that a new request has been made. This request points to\u00a0", ", that looks suspiciously similar to the second request\u00a0that we got when loading the page for the first time (that request got the first\u00a0image on the gallery). So if we look at the\u00a0", "\u00a0file we'll\u00a0find that the image URL for finding its complete data is stored in a\u00a0", "attribute.", "So, to scrape this site, we add the\u00a0", "\u00a0URL to the Spider\u00a0", ", parse it and make requests for each individual image\u00a0(mimicking the requests the browser makes when clicking on images).", "Here's a simple spider to illustrate this:", "You can run this spider quickly (without creating a project) by saving it into a ", " file and running:", "\u00a0\u00a0 \u00a0"], "link": ["http://blog.scrapy.org/scraping-ajax-sites-with-scrapy"], "title": ["Scraping AJAX sites with Scrapy"]},
{"content": ["Hi, for those of you working on Windows, you'll be glad to know that we're now publishing Windows installers for continuous\u00a0builds of Scrapy, courtesy of ", " which provides the continuous building infrastructure. This means installers for the latest code\u00a0committed\u00a0to the Mercurial repo, both for the stable and development series.", "This way you can get the latest bug fixes\u00a0without waiting for the next stable release.", "You can find the Windows installers for latest code\u00a0available\u00a0in the following locations:", "Also remind that the\u00a0", "\u00a0(available in the official APT repo) also contain the latest bug fixes."], "link": ["http://blog.scrapy.org/fresh-windows-installers-for-latest-scrapy-co"], "title": ["Fresh Windows installers for latest Scrapy code"]},
{"content": ["\u00a0"], "link": ["http://blog.scrapy.org/scrapy-0103-released"], "title": ["Scrapy 0.10.3 released"]},
{"content": [], "link": ["http://blog.scrapy.org/new-bugfix-release-0101"], "title": ["New bugfix release: 0.10.1"]},
{"content": ["Hi everyone!", "After two months of work, we're happy to be announcing the release of Scrapy 0.10, the fourth major release after our first 0.7 release almost a year go. We're also using this opportunity to announce the launch the official Scrapy blog, which you are reading right now. And you may have heard of the new ", " site to share code examples.", "The 0.10 release includes several bug fixes, and a bunch of new features. Here we summarize the most important ones.", "There is a new command line tool for creating and controlling your Scrapy projects. Instead of using the old\u00a0", " script you'll now use the ", "command which \"auto discovers\" the project you're working on. So there's no need for a\u00a0separate\u00a0script per project anymore. Check the new ", " for a list of available commands.", "The new command also works out of the box on Windows, if you're using the Windows installer.", "Feed exports provide a flexible way to generate data feeds with the scraped data. It supports multiple formats (JSON, XML, CSV) and storage backends (filesystem, FTP, S3). Both formats and storages are pluggable so you can plug your own one.", "Scrapy now comes with a persistent spider queue (sometimes referred as Execution queues in 0.9) out of the box (implemented using SQLite), which allows you to schedule spiders to a Scrapy process that is already running.", "To illustrate, running this:", "Is the same as running this:", "You can also start Scrapy in server mode with:", "And schedule your spiders later with:", "Scrapy 0.10 introduces\u00a0", ",\u00a0a new application for running Scrapy as a service and deploying Scrapy projects. ", " provides ", "\u00a0(the package source is in the ", " dir) so you can install it with ", "\u00a0and start using it. Hopefully, with some community help, we'll get packages for other platforms.", "New official ", "\u00a0(of Scrapy and Scrapyd) are provided through APT repos which are continuously\u00a0updated with the latest bug fixes. If you install Scrapy through the APT repos you'll get ", " out of the box.", "And much more. For more info see:", "So what are you waiting for?. Go ", ", and please report any issues you find in the ", " group or the ", ". You can also ", " and fix them.", "As usual, you can find the documentation for the previous stable release at\u00a0", "\u00a0and download it from the releases archive at\u00a0", ". The Mercurial repo for the previous stable release will also remain available at\u00a0", " The release files have been rebuilt to fix the issue reported ", " by adding ", "."], "link": ["http://blog.scrapy.org/new-scrapy-blog-and-scrapy-010-release"], "title": ["New Scrapy blog and Scrapy 0.10 release"]},
{"content": ["Last week we proudly released the first release candidate of\u00a0", ", the best web crawling and screen scraping framework for Python.", "For more info see\u00a0", "\u00a0in the\u00a0", ", or head to the\u00a0", ".", "\u00a0"], "link": ["http://blog.scrapy.org/first-scrapy-release-candidate-available"], "title": ["First Scrapy release candidate available"]}][{"content": ["We have decided to shutdown the Scrapy Snippets site.", "The snipets hosted there have been imported into Snipplr and can be accesed here:"], "link": ["http://blog.scrapy.org/scrapy-snippets-shutdown"], "title": ["Scrapy Snippets shutdown"]},
{"content": ["The main reasons for dropping support for Python 2.5 are:", "The code cleanup and the changes haven't been done yet, nor they will happen all at once in a single commit, so Scrapy 0.15 still works on Python 2.5. Not for long however, since we will be making these changes during the remaining development phase of 0.15."], "link": ["http://blog.scrapy.org/scrapy-dropping-support-for-python-25"], "title": ["Scrapy will drop support for Python 2.5"]},
{"content": ["After 10 months of work, and many changes, we are pleased to announce the release of Scrapy 0.14. "], "link": ["http://blog.scrapy.org/scrapy-014-released"], "title": ["Scrapy 0.14 released"]},
{"content": ["Scrapy users have complained in the past about the lack of a pre-built example project that contains, for example, the dmoz spider described in the tutorial.", "Complain no more!. We're happy to let you know that there is a now functional Scrapy project available on Github which contains the old Google Directory spider and the Dmoz spider described in the tutorial.", "The project is called \"dirbot\", and it's available at\u00a0", "\u00a0", "The documentation of Scrapy 0.13 (which will become the next stable release, Scrapy 0.14) has been updated to point to this new example project."], "link": ["http://blog.scrapy.org/new-example-scrapy-project-available"], "title": ["New example Scrapy project available"]},
{"content": ["Hi everyone,", "In an effort to make Scrapy code smaller and more reusable, we\u2019ve been working\non splitting the Scrapy codebase into two different modules:", "A library with simple, reusable functions for working with URLs, HTML, forms,\nand HTTP. Things that aren\u2019t found in the Python standard library. This library\ndoesn\u2019t have any external dependency.", "For more info see:\n* ", "\n* ", "Scrapely is library for extracting structured data from HTML pages. What makes\nit different from other Python web scraping libraries is that it doesn\u2019t depend\non lxml or libxml2. Instead, it uses an internal pure-python parser, which can\naccept poorly formed HTML. The HTML is converted into an array of token ids,\nwhich is used for matching the items to be extracted.", "Scrapely depends on numpy (it uses it to speed up calculations) and w3lib.", "You can find more info, or try it out, in the Github page.", "After these changes, Scrapy codebase has been reduced by 4574 lines, including\nblank and comments (according to ", ").", "Before:", "After:", "Scrapy 0.14 will depend on w3lib. Scrapy 0.13 (current dev version) already\ndepends on w3lib, but w3lib is already packaged and provided in the official\nAPT repos (package python-w3lib). So, if you\u2019re using Scrapy 0.13 on Ubuntu,\nyou can upgrade safely. Otherwise, you can always install/upgrade with\n", " or ", ". Stable version (Scrapy 0.12) is not affected at all by\nthis change.", "If you have any comments or questions feel free to post them in the ", " group."], "link": ["http://blog.scrapy.org/introducing-w3lib-and-scrapely"], "title": ["Introducing: w3lib and scrapely"]},
{"content": ["Hello everyone, we\u2019re pleased to announce the release of Scrapy 0.12!", "This release is based on the last development branch, aka. Scrapy 0.11.", "Starting from this release, we\u2019ll be following the ", ".\nThat means trunk is now Scrapy 0.13 and will became Scrapy 0.14 on next\nrelease.", "Notable changes of this release:", "For a more detailed list of changes, check the ", "."], "link": ["http://blog.scrapy.org/scrapy-012-released"], "title": ["Scrapy 0.12 released"]},
{"content": ["It is well known that many websites show different content depending on the\nregion where they\u2019re accessed. For example, some retailer sites show products\navailable only for the region (US, Europe) of the user accessing the site.", "Although this can be quite convenient for the website customers,\nit can be a pain for developers writing a spider for the site and running it from\ntheir local machines.", "There is a simple way to proxy all requests as if they came from another\nserver. You only need SSH access to this other server, no need to install\nany HTTP proxy. For this, you can use a program called\n", ".", "Here\u2019s how to do it in Ubuntu, though this recipe should be easy to extended to other\nLinux distros.", "First, install tsocks with:", "Then add this content to ", " (this file may vary across distributions):", "Next, SSH to the remote server you want to use:", "And finally, in another terminal (without closing the SSH console), just run Scrapy by prefixing it with the ", " command, like this:", "That\u2019s all. Your spider will run in your local machine but proxying all\ncommunication through the remote server. No need to change any settings or configuration."], "link": ["http://blog.scrapy.org/spoofing-your-scrapy-bot-ip-using-tsocks"], "title": ["Spoofing your Scrapy bot IP using tsocks"]},
{"content": ["A common question in the ", " is how to scrape AJAX sites. As Mark Ellul\u00a0", "\u00a0in the ", " mailing list, there are two basic types of AJAX requests\u00a0that web sites make use of. These are: \"static\" requests which their parameters\u00a0(URL, post data) doesn't change, and \"dynamic\" requests that use some variables\u00a0based on properties from the current page.\u00a0", "The general approach when dealing with \"static\" AJAX requests is adding their\u00a0URLs to ", "attribute as a \"normal\" URL. And to deal with \"dynamic\"\u00a0ones we will try to generate the same requests from Scrapy.", "To help us in this task, we'll use a Firefox add-on called\u00a0", ". This add-on comes with a\u00a0", "\u00a0that let us monitor the requests\u00a0being sent to the server and their responses", "We will scrape\u00a0", ".\u00a0When loading the site we can see that the page loads the gallery information\u00a0from another source, so to find it out we launch Firebug, go to the Net panel\u00a0and reload the page.", "In the Net panel, we see each request (and its response) made to load the entire\u00a0page contents, here we can filter the requests and look for ", " in\u00a0the XHR tab.\u00a0", "In the XHR tab, we see that two requests are made, one to\u00a0", "\u00a0and one\u00a0to\u00a0", ". If we look at the response of the first one\u00a0(clicking on it and then going to response tab) we see that it holds the gallery\u00a0slider data.", "Now, if we navigate to another photography, clicking on its slider link we'll\u00a0see that a new request has been made. This request points to\u00a0", ", that looks suspiciously similar to the second request\u00a0that we got when loading the page for the first time (that request got the first\u00a0image on the gallery). So if we look at the\u00a0", "\u00a0file we'll\u00a0find that the image URL for finding its complete data is stored in a\u00a0", "attribute.", "So, to scrape this site, we add the\u00a0", "\u00a0URL to the Spider\u00a0", ", parse it and make requests for each individual image\u00a0(mimicking the requests the browser makes when clicking on images).", "Here's a simple spider to illustrate this:", "You can run this spider quickly (without creating a project) by saving it into a ", " file and running:", "\u00a0\u00a0 \u00a0"], "link": ["http://blog.scrapy.org/scraping-ajax-sites-with-scrapy"], "title": ["Scraping AJAX sites with Scrapy"]},
{"content": ["Hi, for those of you working on Windows, you'll be glad to know that we're now publishing Windows installers for continuous\u00a0builds of Scrapy, courtesy of ", " which provides the continuous building infrastructure. This means installers for the latest code\u00a0committed\u00a0to the Mercurial repo, both for the stable and development series.", "This way you can get the latest bug fixes\u00a0without waiting for the next stable release.", "You can find the Windows installers for latest code\u00a0available\u00a0in the following locations:", "Also remind that the\u00a0", "\u00a0(available in the official APT repo) also contain the latest bug fixes."], "link": ["http://blog.scrapy.org/fresh-windows-installers-for-latest-scrapy-co"], "title": ["Fresh Windows installers for latest Scrapy code"]},
{"content": ["\u00a0"], "link": ["http://blog.scrapy.org/scrapy-0103-released"], "title": ["Scrapy 0.10.3 released"]},
{"content": [], "link": ["http://blog.scrapy.org/new-bugfix-release-0101"], "title": ["New bugfix release: 0.10.1"]},
{"content": ["Hi everyone!", "After two months of work, we're happy to be announcing the release of Scrapy 0.10, the fourth major release after our first 0.7 release almost a year go. We're also using this opportunity to announce the launch the official Scrapy blog, which you are reading right now. And you may have heard of the new ", " site to share code examples.", "The 0.10 release includes several bug fixes, and a bunch of new features. Here we summarize the most important ones.", "There is a new command line tool for creating and controlling your Scrapy projects. Instead of using the old\u00a0", " script you'll now use the ", "command which \"auto discovers\" the project you're working on. So there's no need for a\u00a0separate\u00a0script per project anymore. Check the new ", " for a list of available commands.", "The new command also works out of the box on Windows, if you're using the Windows installer.", "Feed exports provide a flexible way to generate data feeds with the scraped data. It supports multiple formats (JSON, XML, CSV) and storage backends (filesystem, FTP, S3). Both formats and storages are pluggable so you can plug your own one.", "Scrapy now comes with a persistent spider queue (sometimes referred as Execution queues in 0.9) out of the box (implemented using SQLite), which allows you to schedule spiders to a Scrapy process that is already running.", "To illustrate, running this:", "Is the same as running this:", "You can also start Scrapy in server mode with:", "And schedule your spiders later with:", "Scrapy 0.10 introduces\u00a0", ",\u00a0a new application for running Scrapy as a service and deploying Scrapy projects. ", " provides ", "\u00a0(the package source is in the ", " dir) so you can install it with ", "\u00a0and start using it. Hopefully, with some community help, we'll get packages for other platforms.", "New official ", "\u00a0(of Scrapy and Scrapyd) are provided through APT repos which are continuously\u00a0updated with the latest bug fixes. If you install Scrapy through the APT repos you'll get ", " out of the box.", "And much more. For more info see:", "So what are you waiting for?. Go ", ", and please report any issues you find in the ", " group or the ", ". You can also ", " and fix them.", "As usual, you can find the documentation for the previous stable release at\u00a0", "\u00a0and download it from the releases archive at\u00a0", ". The Mercurial repo for the previous stable release will also remain available at\u00a0", " The release files have been rebuilt to fix the issue reported ", " by adding ", "."], "link": ["http://blog.scrapy.org/new-scrapy-blog-and-scrapy-010-release"], "title": ["New Scrapy blog and Scrapy 0.10 release"]},
{"content": ["Last week we proudly released the first release candidate of\u00a0", ", the best web crawling and screen scraping framework for Python.", "For more info see\u00a0", "\u00a0in the\u00a0", ", or head to the\u00a0", ".", "\u00a0"], "link": ["http://blog.scrapy.org/first-scrapy-release-candidate-available"], "title": ["First Scrapy release candidate available"]}]