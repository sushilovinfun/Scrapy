body
"Hello booklovers,"
My name is 123dutch.
I am from the Netherlands. And I am co-admin at uz-translations.net.
Our website specialises in everything to learn a language.
On uz-translations we are ideologists. We think that information should be free.
"We would like everyone regardless of origin, religion, status and what more, to be able to gather the information they need."
"We believe that to prevent books from dissapearing, they must be spread as much as possible."
"We have a few limitations: we do not publish material on our website that IS already available for everyone at very low cost. Because we believe that if all publishers would be like that, we would not be needed anymore. Which would be a good thing."
Example: dutchgrammar.com is a dutch website where for something like 2 euro you can download everything from there to learn the dutch language - or 
 where you can also for very little money download everything you need. 
Recently I was able to download the complete LG through torrents. We installed it on our server using the same webinterface you are using. Our server is used for backup purpose of our website(s). So it only runs a localhost of LG.
So everything from 0 - 874000 is there now.
Then came our idea how to prevent all this to be lost in case of an emergency. Because harddisks can crash. Servers can be taken down. Torrents at LG can end. So how to make that extra backup in case of such a disaster?
Downloading all through torrents again would take too long and one would never know what can happen is such a long period.
In the Netherlands Usenet is very common to use. It has a few advantages over torrents - and also a few disadvantages. But for OUR goal it would fit perfectly.
The disadvantages: it is not possible to download 1 single book - you can only download 1 book at a time / you need a payed usenetaccount to acces the files on Usenet.
The advantages: the files are stored on Usenet for 4.5 years before they are deleted / you can download from usenet at the speed of your internetconnection / uploading and downloading can be done anonymous.
Simultaneously we are reaching out to become more friends with communities that share the same ideology as we do. So we reached out to become friends with LG. And here we are now.
We know that there are many people that would like to download to entire collection. And from there-on spread further.
"And this could also be a good way to have an EXTRA way for downloading everything, next to torrents. But as said earlier, you cannot just pick 1 book - only 1000 at a time."
Downloading the entire 9TB collection over Usenet takes at most 2 weeks - depending on your internetconnection ofcourse. You can download to 100MB/s.
Uploading the entire LG to Usenet will take me about 3 months. End of march I hope to be ready.
Files are uploaded exactly as the contents of the torrents. With 1000 books. With the same foldernames. With the original MD5.
"It is YOUR library so we will only add the files on our forum - no where else (only if you want us to) - and we will give the files to download ofcourse to you here at LG. If you wish to add them to your website or spread them further, I would only like to encourage that."
Let''s take folder 619000 per example. Everthing in the folder is rarred in pieces of 1GB. Added are par2 files (used on usenet to repair the rars). On usenet little things go wrong with uploading sometimes and the combination of RAR and PAR is used to be able to repair if neccesary and download in 100% exact state. The namegiving must always be unique - else finding things on usenet is difficult - so the generic namegiving I chose is: gen619000esis (and for 618000 this is ofcourse gen618000esis). 
"This gives then the following files: gen619000esis.part01.rar, gen619000esis.part02.rar etc PLUS gen619000esis.par2, gen619000esis.vol000+01.PAR2 etc. "
If you search on usenet you will find all the files together when you search for gen619000esis
Let''s use binsearch.info to search gen619000esis:
"You see here the entire 619000 collection: 9 par2 files, 12 rar, 1 NZB."
"Currently the publishers have their arrows on Usenet search engines, like binsearch.info."
"In the rare event that all usenet search engines would end, there is still the NZB file. The NZB file (like a .torrent) contains all the files  as you see when you would search for the files. Meaning that with downloading you will not need a search engine at all. Only the NZB file will be enough. You simply add the NZB into your newsreadingprogramm and the download starts automatically."
I am aware that downloading with Usenet is not so common in Russia. So should you have more questions feel free to ask. there is much more to explain if needed.
Here is more info how usenet works:
And lastly: thank you very much for welcoming me into your home 
"Kind regards,"
123dutch
"hi 123dutch,"
this is very interesting! Thank you for spreading the books.
"some people tried to download files from uz-translations, and one problem with uz-translations is that there is no way to download the entire collection of files. Is it available on usenet? Is there a list of all files, or some kind of database where all files are listed?"
"Dear are,"
We are in the middle of a process in centralising the data.
At this moment  - other than one by one - it is not possible to download the entire collection.
"After the proces this will be a little easier, but still dificult. "
Many data on uz contains of books+audio. Should you want to download everything then we are talking about 50TB+ on data.
LG would probably be only interested in the books - not the audio. And then even only the books that are not on LG yet.
The centralising may take 4-6 months.
"And we need some time to adjust to that idea as well. Sharing and spreading data IS eminent, but we are undergowing huge changes ourselves at the moment (all good) and after the changes we need to settle-in a little bit. Then we can think what would be the best way."
"For the time being most of our data is already protected - I am saying """"most""""  because this is what the centralising of our data implies to begin with."
from our 
" only uz-trans whoud have maximally 300 GB of textual books (no audio etc), correcting for the excessive URLs in the uz-trans blog the number reduces to about 100 GB. It is harder to say how many intersection LG may have with uz-trans, but I expect it's a lot, more than 1000 books, which is a huge collection if to speak about a professional/dedicated topical library, we have at least one remarkable example collected by plantago on botany. Realistically we should expect probably "
", because the number of our books on linguistics is probably about 2k. This is only from uz-trans."
"welcome, and feel at home!"
повесил на главной объявления.
Personally I expect more than 300GB.
"In the process of getting all links centralised, LG is one of the main sites for us to check when files are missing to bring back to UZ, and sections as linguistic are often found here on LG, but specific language learning books often not."
"My estimate would be more like 25000 books are missing on LG from UZ (not counting greylib, turklib)."
The average post on uz has 2 books. We have 40.0000+ posts. Of these 40.000+ at least 50% is true language learning material.
"This seems much, but it is what i see every day."
It does not matter so much.
"I am thinking about a system how to get the books only, to LG."
I have no idea if one of you has a high speed internetconnection. Else (after the books are centralised) I could set up a torrent file to 1 person here with all the needed books in. Something like that. Other ideas are welcome.
But - though I believe this must be done - many people on uz still need to get used to that idea. So give it a little time.
"What I AM seeing is only positive changes at UZ, and a BIG increase on new visitors daily."
"We have 380.000 registered members, and the number of active ones is unknown. I am guessing 10%. But as an example, 8 books of Cortina were published today (a must have for LG in my opinion by the way) and after 10 minutes online we already counted 100 completed downloads. This gives me a little idea of how active the community is."
"The transformation of the idea that books should spread is gradually spreading through other members. But for some people little steps are needed. Too big steps at once, and they will get scared with all the change. We admins LOVE the change by the way. We embrace it completely. And the path to take is obvious and inevitable to keep the books alive."
"uz-trans is a beautiful resource, it's obvious that people would wish to dwell there, unlike in LG. It is usually the case: if the project is liveable, it has a large permanent community. We observe the same effect with lib.rus.ec, flibusta.net, awaxhome etc etc. Humans need home, not just a beautiful engine."
"On the side of the structure, I wish you regularize it in future for simpler handling, then centralization won't be a pain any longer. Of course when the project has been running for years with an unstructured accounting of the book records and files, it's gonna be quite an effort to first sort out the problems with the old records that need to be accurately collected etc. Since I've been trying to parse your site, I can say it is very hard and should mainly be done by hands. A very little effort in the beginning would've eliminated the need to heat the ocean at present. I mean the fields of a book record should've been fixed and the files should've been rigirously linked to the records. That's what we have in LG and, in fact, nothing else. The rest is we track that this link between the file and the record is not broken."
It is true - we have had no structure at all for many many years - so everything IS done right now to manually get everything centralised.
"Yes, this is much work. Hence the 4-6 months we expect to need for this. But after this things will be in control. Still much optimilisation will be needed then, but we will be ready to spread the neccesary material to LG."
But this is done by multiple hands - not only 1 anymore - all working for the same goal. 
"When it comes to further optimilisation after we centralised everything, this will be still an imense job. Since many of our posts need to have a interactive design because newer posts/better posts/completer posts to an existing post are added on a daily basis, it is more difficult to bring structure without losing the flexibility. For books only we could use isbn or something like that, but since in our case we are talking about multimedia material as well -and the for us needed coherency between the books and the added material - it needs a more complex design. "
Currently we have no other option but categorising per language.
"I'd split the categorization issues from the core part: how to store the record and the file - those two are the core, the rest should come along. I think it would help you a lot, if you prioritize the structural features: flexibility on the Web may stay exactly the same while the core information is collected in a rigorous/stable way. For example, if each book-thread has a field to enter URLs relevant to the book and a flag to show which records are out-of-date, you are done, your database will be tracking only the most recent information. It's sketchy, but the way of collecting links must be exceptionally robust. Even if you do not mark up the old records as obsolete, the collection of all links rigirously is already a big deal. In the most trivial way it may look as the HTML textarea. The posts may additionally be filtered for any presence of URLs to make the user fill in the textarea instead."
"The thread head would needs some fixed set of field, as usual in bibliography. Users should not have unnecessary degrees of freedom at filling the records, it should all be templated. Otherwise they'll be entering anything they like in any format."
"Though, it'll be difficult to make all fields rigorous, and absolute strictness will discourage many ot upload at all. Because bibliographic records are very complex in their general form. Many fields would need to be left for free style completion. For this reason in LG there are fields that are "
" changed from the Web, they are only displayed in read-only mode: ID, MD5, filesize and file extension. Those are vital for the collection integrity and must not be changed. Everything else can be changed. I guess in your case you'd also need to distinguish what must be strict and what must not be, for the sake of flexibility."
I agree that something or some things NEEDS to have a status that never change.
We will need to figure out how to do that. Because currently even POSTS itself need to be deleted quite regularly in order to have the most actual information present on the website.
"If we were to make a system with parts that never change, then we really would need to start building from scratch. It can be done."
"But this would take NOW more manpower than we can handle. So for the time being - till a major website upgrade is planned anyway - we have to at least gather all material centralised. Which could have been done more effectively if we had a system like you wrote out, true."
When UZ started in 2004 the needed information on every post was much different from now. No one expected to grow so much.
"So over the years items were more and more difficult to find - we implemented some changes - but never from scratch. So in the website you see posts from 2004 which are already differenly structured that posts from 2008, and so on."
Thanks for you insights 
"from scratch maybe an overkill, just a replacement of the head message code in a book thread should suffice, one PHP-script, like in our case for the Librarian interface: "
(genesis:upload)
"you can see the forbidden to modify entries as the uneditable fields below, for an example book "
"if something functionally similar appears for each book, the rest of the site can be kept as is. The well known Open Library has a similar interface, when I first saw it, it was amazingly similar to our Librarian GUI. It's a good indication that other ways may miss something important. OL has invested a lot of efforts to a dedicated database engine to work with records, but the GUI hasn't gotten much more complex than ours (which is quite simple, but fills in a regular database structure)."
"after looking to the insides of uz-trans and seeing how the books get to the collection, it stroke me that it would be good to have a global book collector site with inified upload process that distributes books over the libraries registered in it (and supporting the protocol). Because it seems that everyone is fighting with the same set of technical problems, and many large libraries could've been much more efficient, if they had some protocols. I mean that releasing books in forum threads is a heavy approach: the book needs to be processed by many, manually. The post-moderation approach works fine too."
"Many different specialized libraries is very good, but they need to follow some protocol, then humans can do what they do best, and the mighty ""computer"" (the global marshalling portal) will be doing the best it can: distributing books to different libraries, but keeping track of all of them in the database."
"Of course, the database will be much-much more simple than that of OpenLibrary, e.g., but the number of electronically available books suggests that such a way is a lot more productive than making a perfect database without book files properly attached to it."
@123dutch
"Thank you very much for this! I am unable to use torrents as my service provider throttles/shapes all p2p traffic (I get 1 KB/s at best). This makes it entirely impractical to  download a multi-terabyte archive via torrents. Usenet, on the other hand, runs at full speed. You have made it possible for me to mirror libgen! (I have quite limited speed so this is still going to take 6-8 months -- better than 6-8 YEARS though.)"
I do have one suggestion:
"Would you consider increasing the percentage of PAR files? You are currently providing about 5% par2 recovery files. In one case (the gen10000esis fileset), using astraweb as my usenet provider, I was unable to recover from checksum mismatch errors -- 67 additional blocks were still required for repair. In this case I was able  to  successfully re-download this fileset from my ISP-provided newsserver (*very slow*) which gets its data from gigagnews. "
It would be good to make the Usenet backup of libgen as robust as possible; increasing the percentage of available PAR2 files would go a long way to achieving this. While this would increase your upload times by 5-10%  it would greatly increase the chances of successfully downloading libgen material without having to use backup servers.
"In any case, you're doing the world a great service. Thank you again!"
@bookwarrior: Of course I also have to thank you and the libgen crew for creating this amazing library. I am sure thousands all over the world think kind thoughts for you and all your efforts.
"Dear ansaron,"
Thanks very much for your reply. It is nice to see that the NZB''s are actually used 
"Some newer NZB''s have been uploaded already, but I will increase par2 to 10%, starting with gen108000essis ."
"Kind regards,"
123dutch
"Dear 123dutch,"
I'm sure there are many others out there making use of your Usenet uploads too -- and numbers will increase as word about the availability of the libgen archive via Usenet gets out. I do appreciate your decision to increaser the percentage of par2 repair files; Thanks!
"I am now systematically downloading the available archives and came across one unusual error: In downloading the gen22000isis fileset  there were some checksum erros, but these were repairable by the parity files. However, the resulting repaired rar files were then reported as being corrupt. On OS X  the decompression tools refused to extract files, reporting either a corrupted  or passworded archive. I moved the rar files to a windows machine which extracted 998 out of 1000 files, reporting two files as  being corrupt:"
!   I:\1358885413\gen22000esis.part01.rar: CRC failed in 22000\00b010f787b357e24133a8b976650e67. The file is corrupt
!   I:\1358885413\gen22000esis.part04.rar: CRC failed in 22000\650c08e11b676a4dc2f5ede8c8287a4c. The file is corrupt
"I downloaded the archive twice; once using astraweb and once using TweakNews, so I don't think this is a Usenet provider problem (in any case the archive has been successfully repaired so the problem is deeper.) Could you check on this?"
"One additional thing: you're splitting up the rar files at the 1GB level; this is (based at least on my experience of Usenet) unusually large. Is there a reason for this? Is there slightly less overhead in uploading a smaller number of files? The reason I ask is that I'm getting more checksum errors than I'm used to seeing. Could it be that the very large size of the rar files is responsible? Would it be a burden to decrease the size of each rar file to, say, 100MB? (My usual Usenet provider astraweb is normally quite reliable -- except when they overeagerly obey DMCA takedown orders! --  so it's a bit worrying.)"
"Please do let me know if you want these sort of reports, or if you'd prefer me to shut up  "
.
"In any case, with great gratitude,"
ansaron
"Haha, no please keep the remarks coming "
"1GB is big, but since some archives are 20G I thought we would get so many files... I can make the rar''s smaller if that helps."
"I wonder if that changes the outcome. I am using giganews so I have no idea if that makes a difference towards other providers. 100MB on the other hand seems so small, since you would get really many files."
Now I am not verifying the rar. I will from now on.
Now I am not adding a recovery archive to the rar. Maybe I should.
So I would like to hear your opinion on number 2 and 3:
"1. I raised the par2 to 10% already <-- good for recovering, but the 9TB size gets 5% bigger. (it is a choice to download all par2 files - so this is not a real problem - if you have giganews you will almost never need the par2 files)"
"2. I can add the recovery archive <-- this would increase the chance of recovering (if used well), but also this would increase the total size of the archive. (this increases the total size for everyone)"
3. making smaller sizes like 100MB parts instead of 1GB parts would really help?
"kind regards,"
123dutch
Good to know your original Usenet provider; as many of the checksum errors are introduced in the mirroring process Giganews might be a good choice for people intent on downloading libgen.  I was having a particularly bad time with astraweb. After I switched to TweakNews the number of corrupted files went down. I will certainly try GigaNews next.
In response to your points:
"(1) Yes, it is a choice, and in fact some nzb clients are able to automatically decide if the par2 files are even required. Since some people will be using substandard ISP-provided nntp servers being generous with par files seems to be a very good idea. "
(2) I think we should think in terms of maximizing the longterm robustness of  the Usenet backup of libgen. Since these files are hopefully going to be on Usenet servers for 4+ years (hopefully even longer as Usenet providers keep on extending their retention levels) we should aim at providing files that are are almost guaranteed to flawlessly decompress. Taking that longterm view means that an additional 5-10% downloading time is not a significant demerit. I vote for recovery archives.
(3) I honestly don't know. I have absolutely no experience of uploading to Usenet. My comments were based only on my extensive Usenet downloading. For now keep things as they are. I'm going to be using your NZBs 24/7 from today. If I encounter too frequent errors I'll let you know and you can try and adjust the RAR  part size down.
A question:  do you have this set up as some sort of automated cron job or are you manually preparing these uploads?  
"One last comment: you started posting the libgen archive with the ID CPP-gebruiker, then recently switched to CPP-user (I know enough Dutch to know that's the same thing "
" ).  The last libgen archive that you posted as CPP-gebruiker was gen105000esis which after 2 days is still incomplete, while later archives are complete. Is there possibly a problem with the gen105000esis upload?       "
"As always, my thanks and kindest regards,"
A.
Thanks for your comments again - they are welcomed.
I will make the adjustments for next uploads. 
All is done manually. So I can simply make any adjustments without problem of things not uploaded yet. And if we encounter things that could have been uploader BETTER I will re-upload them.
"ccp-user vs ccp-gebruiker: I was using my work-connection to upload since it much faster at that moment, and the upload killed my laptop uploading the gen105000eses. Killed the programm I use for uploading also (camelsystempowerpost). I tried some tricks - changing from the dutch to english version of the programm a few times - and it took me 4 days or so to finally find the problem. The user/gebruiker is from me using the english or dutch programm to upload. 105000 is still to be uploaded. I thought at some point that the error might be in the 105000, but it was not."
"I uploaded the 106, 107 and 108 already. Will continue with the 105 next, using the new settings."
Please keep reporting so I have a list of things that need to be re-upped or not.
Thanks.
123dutch
123dutch
"have you actually check MD5's before uploading to Usenet? It'll probably take a week, but it's worth doing, from our practice it always discovers surprizes. HDDs are imperfect, possibly 1 byte per billion isn't written correctly etc. Once the files settle in some machine it is desirable to do such a check once."
Good addition.
Texas is checking them now.
"He started with 110.000 and up at my request now, before rarring them."
"Those are all identical (except for a few files missing  - about 15 files or so are missing from the 9TB because they were infected (or most likely infected) by a virus - so IF you or anyone else sees an archive not containing 1000 but 998 books, then THAT is the reason)."
"So far all checks out  - the server we are using is quite fast, so checking md5 will take about 6 minutes per 1000 (88hours) - all will be checked before uploading now. If we encounter an md5 error in the ones uploaded already, then I will re-up."
@Bookwarrior: 
It is really nice that you use the MD5 hash as the filename in libgen -- it makes the checking of a file's integrity self-contained.
If people are going to check the MD5s of libgen archives they might want to look at md5deep (
). This has some very nice features compared to the usual md5sum program. The ones relevant to us are:
123dutch
"somewhen after you filter out the suspecious infected files, we'd appreciate the listing: in most cases in the past it was false positive, but there are many executables in LG yet (man power problem). Although 1 in a 1000 looks quite wrong, especially if it's in the first 100k. But it's all possible of course."
All archives except gen22000esis expanded correctly. 
"In gen22000esis the rar archive was reported as being corrupted. 998/1000 files were extracted, with the remaining two being reported as corrupt. In addition md5deep found a hash mismatch for the file 22000/f54a3bce73a27a235960db70c636aded  "
Conclusion: gen22000esis needs to be re-upped.
I will post reports on 123dutch's Usenet uploads (including MD5 verification) in batches of 25000.
 gen22000esis-repost.nzb is perfect. It decompresses without error and there are no MD5 mismatches.
perfect!!! Thanks a lot!
"The 22000 is one of the archives before I verified rar''s... I downloaded this one, extracted, and it gives 2 crc errors."
"The 22000 will be reupped today, name: gen22000esis-repost  , but ofcourse the nzb will be added to the repository as well."
Texas is busy verifying all archives for missing books. Also for MD5 mismatches. He will probably finish today or tomorrow and report his findings here.
Note: today I encountered the first time with creating par2 files that quickpar gave an error creating the files. I did my usually tests and tricks  - and sometimes changing the settings in quickpar could fix it - but the bigger the archive the more difficult to fix. On the net is explained that when you have too much internal memory quickpar can give these kind of errors. The only one that I could not make par files for was 121000 so far (which is a huge archive of 33GB) - not even with another programm. But on my laptop I have only 4GB so I am making it there with no problem (just takes very long)
