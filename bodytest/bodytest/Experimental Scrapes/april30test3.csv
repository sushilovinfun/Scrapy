topic_title,topic_url,post_author,post_body,thread_author
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,are,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,Bill_G,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,ansaron,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,ansaron,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,ansaron,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,ansaron,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,ansaron,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,bookwarrior,,123dutch
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&t=6769&sid=700ac7fb1d771b090dc52852f0751fd1,123dutch,,123dutch
,,,"<div class=""postbody"">Hello booklovers,<br><br>My name is 123dutch.<br>I am from the Netherlands. And I am co-admin at uz-translations.net.<br>Our website specialises in everything to learn a language.<br>On uz-translations we are ideologists. We think that information should be free.<br>We would like everyone regardless of origin, religion, status and what more, to be able to gather the information they need.<br>We believe that to prevent books from dissapearing, they must be spread as much as possible.<br><br>We have a few limitations: we do not publish material on our website that IS already available for everyone at very low cost. Because we believe that if all publishers would be like that, we would not be needed anymore. Which would be a good thing.<br>Example: dutchgrammar.com is a dutch website where for something like 2 euro you can download everything from there to learn the dutch language - or <!-- m --><a class=""postlink"" href=""http://www.book2.de/"">http://www.book2.de/</a><!-- m --> where you can also for very little money download everything you need. <br><br><br>Recently I was able to download the complete LG through torrents. We installed it on our server using the same webinterface you are using. Our server is used for backup purpose of our website(s). So it only runs a localhost of LG.<br>So everything from 0 - 874000 is there now.<br><br>Then came our idea how to prevent all this to be lost in case of an emergency. Because harddisks can crash. Servers can be taken down. Torrents at LG can end. So how to make that extra backup in case of such a disaster?<br>Downloading all through torrents again would take too long and one would never know what can happen is such a long period.<br><br>In the Netherlands Usenet is very common to use. It has a few advantages over torrents - and also a few disadvantages. But for OUR goal it would fit perfectly.<br>The disadvantages: it is not possible to download 1 single book - you can only download 1 book at a time / you need a payed usenetaccount to acces the files on Usenet.<br>The advantages: the files are stored on Usenet for 4.5 years before they are deleted / you can download from usenet at the speed of your internetconnection / uploading and downloading can be done anonymous.<br><br>Simultaneously we are reaching out to become more friends with communities that share the same ideology as we do. So we reached out to become friends with LG. And here we are now.<br><br>We know that there are many people that would like to download to entire collection. And from there-on spread further.<br>And this could also be a good way to have an EXTRA way for downloading everything, next to torrents. But as said earlier, you cannot just pick 1 book - only 1000 at a time.<br>Downloading the entire 9TB collection over Usenet takes at most 2 weeks - depending on your internetconnection ofcourse. You can download to 100MB/s.<br><br>Uploading the entire LG to Usenet will take me about 3 months. End of march I hope to be ready.<br>Files are uploaded exactly as the contents of the torrents. With 1000 books. With the same foldernames. With the original MD5.<br>It is YOUR library so we will only add the files on our forum - no where else (only if you want us to) - and we will give the files to download ofcourse to you here at LG. If you wish to add them to your website or spread them further, I would only like to encourage that.<br><br><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">How it works</span></span><br>Let''s take folder 619000 per example. Everthing in the folder is rarred in pieces of 1GB. Added are par2 files (used on usenet to repair the rars). On usenet little things go wrong with uploading sometimes and the combination of RAR and PAR is used to be able to repair if neccesary and download in 100% exact state. The namegiving must always be unique - else finding things on usenet is difficult - so the generic namegiving I chose is: gen619000esis (and for 618000 this is ofcourse gen618000esis). <br>This gives then the following files: gen619000esis.part01.rar, gen619000esis.part02.rar etc PLUS gen619000esis.par2, gen619000esis.vol000+01.PAR2 etc. <br>If you search on usenet you will find all the files together when you search for gen619000esis<br>Let''s use binsearch.info to search gen619000esis:<br><!-- m --><a class=""postlink"" href=""http://binsearch.info/?q=gen619000esis&amp;max=100&amp;adv_age=1100&amp;server="">http://binsearch.info/?q=gen619000esis&amp; ... 00&amp;server=</a><!-- m --><br>You see here the entire 619000 collection: 9 par2 files, 12 rar, 1 NZB.<br><br>Currently the publishers have their arrows on Usenet search engines, like binsearch.info.<br>In the rare event that all usenet search engines would end, there is still the NZB file. The NZB file (like a .torrent) contains all the files  as you see when you would search for the files. Meaning that with downloading you will not need a search engine at all. Only the NZB file will be enough. You simply add the NZB into your newsreadingprogramm and the download starts automatically.<br><br>I am aware that downloading with Usenet is not so common in Russia. So should you have more questions feel free to ask. there is much more to explain if needed.<br><br>Here is more info how usenet works:<br><!-- m --><a class=""postlink"" href=""http://www.binaries4all.com/"">http://www.binaries4all.com/</a><!-- m --><br><br>And lastly: thank you very much for welcoming me into your home <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br><br>Kind regards,<br>123dutch</div>",
,,,"<div class=""postbody"">hi 123dutch,<br><br>this is very interesting! Thank you for spreading the books.<br><br>some people tried to download files from uz-translations, and one problem with uz-translations is that there is no way to download the entire collection of files. Is it available on usenet? Is there a list of all files, or some kind of database where all files are listed?</div>",
,,,"<div class=""postbody"">Dear are,<br><br>We are in the middle of a process in centralising the data.<br>At this moment  - other than one by one - it is not possible to download the entire collection.<br>After the proces this will be a little easier, but still dificult. <br>Many data on uz contains of books+audio. Should you want to download everything then we are talking about 50TB+ on data.<br>LG would probably be only interested in the books - not the audio. And then even only the books that are not on LG yet.<br><br>The centralising may take 4-6 months.<br>And we need some time to adjust to that idea as well. Sharing and spreading data IS eminent, but we are undergowing huge changes ourselves at the moment (all good) and after the changes we need to settle-in a little bit. Then we can think what would be the best way.<br>For the time being most of our data is already protected - I am saying """"most""""  because this is what the centralising of our data implies to begin with.</div>",
,,,"<div class=""postbody"">from our <span style=""font-weight: bold""><a href=""http://genofond.org/viewtopic.php?p=25006#p25006"" class=""postlink"">previous discussion</a></span> only uz-trans whoud have maximally 300 GB of textual books (no audio etc), correcting for the excessive URLs in the uz-trans blog the number reduces to about 100 GB. It is harder to say how many intersection LG may have with uz-trans, but I expect it's a lot, more than 1000 books, which is a huge collection if to speak about a professional/dedicated topical library, we have at least one remarkable example collected by plantago on botany. Realistically we should expect probably <span style=""font-weight: bold"">5000-8000 new books</span>, because the number of our books on linguistics is probably about 2k. This is only from uz-trans.<br><br>welcome, and feel at home!</div>",
,,,"<div class=""postbody"">повесил на главной объявления.</div>",
,,,"<div class=""postbody"">Personally I expect more than 300GB.<br>In the process of getting all links centralised, LG is one of the main sites for us to check when files are missing to bring back to UZ, and sections as linguistic are often found here on LG, but specific language learning books often not.<br>My estimate would be more like 25000 books are missing on LG from UZ (not counting greylib, turklib).<br>The average post on uz has 2 books. We have 40.0000+ posts. Of these 40.000+ at least 50% is true language learning material.<br>This seems much, but it is what i see every day.<br><br>It does not matter so much.<br>I am thinking about a system how to get the books only, to LG.<br>I have no idea if one of you has a high speed internetconnection. Else (after the books are centralised) I could set up a torrent file to 1 person here with all the needed books in. Something like that. Other ideas are welcome.<br>But - though I believe this must be done - many people on uz still need to get used to that idea. So give it a little time.<br><br>What I AM seeing is only positive changes at UZ, and a BIG increase on new visitors daily.<br>We have 380.000 registered members, and the number of active ones is unknown. I am guessing 10%. But as an example, 8 books of Cortina were published today (a must have for LG in my opinion by the way) and after 10 minutes online we already counted 100 completed downloads. This gives me a little idea of how active the community is.<br><br>The transformation of the idea that books should spread is gradually spreading through other members. But for some people little steps are needed. Too big steps at once, and they will get scared with all the change. We admins LOVE the change by the way. We embrace it completely. And the path to take is obvious and inevitable to keep the books alive.</div>",
,,,"<div class=""postbody"">uz-trans is a beautiful resource, it's obvious that people would wish to dwell there, unlike in LG. It is usually the case: if the project is liveable, it has a large permanent community. We observe the same effect with lib.rus.ec, flibusta.net, awaxhome etc etc. Humans need home, not just a beautiful engine.<br><br>On the side of the structure, I wish you regularize it in future for simpler handling, then centralization won't be a pain any longer. Of course when the project has been running for years with an unstructured accounting of the book records and files, it's gonna be quite an effort to first sort out the problems with the old records that need to be accurately collected etc. Since I've been trying to parse your site, I can say it is very hard and should mainly be done by hands. A very little effort in the beginning would've eliminated the need to heat the ocean at present. I mean the fields of a book record should've been fixed and the files should've been rigirously linked to the records. That's what we have in LG and, in fact, nothing else. The rest is we track that this link between the file and the record is not broken.</div>",
,,,"<div class=""postbody"">It is true - we have had no structure at all for many many years - so everything IS done right now to manually get everything centralised.<br>Yes, this is much work. Hence the 4-6 months we expect to need for this. But after this things will be in control. Still much optimilisation will be needed then, but we will be ready to spread the neccesary material to LG.<br>But this is done by multiple hands - not only 1 anymore - all working for the same goal. <br><br>When it comes to further optimilisation after we centralised everything, this will be still an imense job. Since many of our posts need to have a interactive design because newer posts/better posts/completer posts to an existing post are added on a daily basis, it is more difficult to bring structure without losing the flexibility. For books only we could use isbn or something like that, but since in our case we are talking about multimedia material as well -and the for us needed coherency between the books and the added material - it needs a more complex design. <br>Currently we have no other option but categorising per language.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent"">For books only we could use isbn or something like that, but since in our case we are talking about multimedia material as well -and the for us needed coherency between the books and the added material - it needs a more complex design. <br>Currently we have no other option but categorising per language.</div>I'd split the categorization issues from the core part: how to store the record and the file - those two are the core, the rest should come along. I think it would help you a lot, if you prioritize the structural features: flexibility on the Web may stay exactly the same while the core information is collected in a rigorous/stable way. For example, if each book-thread has a field to enter URLs relevant to the book and a flag to show which records are out-of-date, you are done, your database will be tracking only the most recent information. It's sketchy, but the way of collecting links must be exceptionally robust. Even if you do not mark up the old records as obsolete, the collection of all links rigirously is already a big deal. In the most trivial way it may look as the HTML textarea. The posts may additionally be filtered for any presence of URLs to make the user fill in the textarea instead.<br><br>The thread head would needs some fixed set of field, as usual in bibliography. Users should not have unnecessary degrees of freedom at filling the records, it should all be templated. Otherwise they'll be entering anything they like in any format.<br><br>Though, it'll be difficult to make all fields rigorous, and absolute strictness will discourage many ot upload at all. Because bibliographic records are very complex in their general form. Many fields would need to be left for free style completion. For this reason in LG there are fields that are <span style=""font-weight: bold"">never</span> changed from the Web, they are only displayed in read-only mode: ID, MD5, filesize and file extension. Those are vital for the collection integrity and must not be changed. Everything else can be changed. I guess in your case you'd also need to distinguish what must be strict and what must not be, for the sake of flexibility.</div>",
,,,"<div class=""postbody"">I agree that something or some things NEEDS to have a status that never change.<br>We will need to figure out how to do that. Because currently even POSTS itself need to be deleted quite regularly in order to have the most actual information present on the website.<br><br>If we were to make a system with parts that never change, then we really would need to start building from scratch. It can be done.<br>But this would take NOW more manpower than we can handle. So for the time being - till a major website upgrade is planned anyway - we have to at least gather all material centralised. Which could have been done more effectively if we had a system like you wrote out, true.<br><br>When UZ started in 2004 the needed information on every post was much different from now. No one expected to grow so much.<br>So over the years items were more and more difficult to find - we implemented some changes - but never from scratch. So in the website you see posts from 2004 which are already differenly structured that posts from 2008, and so on.<br><br>Thanks for you insights <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",
,,,"<div class=""postbody"">from scratch maybe an overkill, just a replacement of the head message code in a book thread should suffice, one PHP-script, like in our case for the Librarian interface: <!-- m --><a class=""postlink"" href=""http://libgen.org/librarian/"">http://libgen.org/librarian/</a><!-- m --><br>(genesis:upload)<br><br>you can see the forbidden to modify entries as the uneditable fields below, for an example book <!-- m --><a class=""postlink"" href=""http://libgen.org/librarian/registration?md5=00b800ea9aee39af51a6c0d980c74398"">http://libgen.org/librarian/registratio ... d980c74398</a><!-- m --><br><br>if something functionally similar appears for each book, the rest of the site can be kept as is. The well known Open Library has a similar interface, when I first saw it, it was amazingly similar to our Librarian GUI. It's a good indication that other ways may miss something important. OL has invested a lot of efforts to a dedicated database engine to work with records, but the GUI hasn't gotten much more complex than ours (which is quite simple, but fills in a regular database structure).</div>",
,,,"<div class=""postbody"">after looking to the insides of uz-trans and seeing how the books get to the collection, it stroke me that it would be good to have a global book collector site with inified upload process that distributes books over the libraries registered in it (and supporting the protocol). Because it seems that everyone is fighting with the same set of technical problems, and many large libraries could've been much more efficient, if they had some protocols. I mean that releasing books in forum threads is a heavy approach: the book needs to be processed by many, manually. The post-moderation approach works fine too.<br><br>Many different specialized libraries is very good, but they need to follow some protocol, then humans can do what they do best, and the mighty ""computer"" (the global marshalling portal) will be doing the best it can: distributing books to different libraries, but keeping track of all of them in the database.<br><br>Of course, the database will be much-much more simple than that of OpenLibrary, e.g., but the number of electronically available books suggests that such a way is a lot more productive than making a perfect database without book files properly attached to it.</div>",
,,,"<div class=""postbody"">@123dutch<br><br>Thank you very much for this! I am unable to use torrents as my service provider throttles/shapes all p2p traffic (I get 1 KB/s at best). This makes it entirely impractical to  download a multi-terabyte archive via torrents. Usenet, on the other hand, runs at full speed. You have made it possible for me to mirror libgen! (I have quite limited speed so this is still going to take 6-8 months -- better than 6-8 YEARS though.)<br><br>I do have one suggestion:<br>Would you consider increasing the percentage of PAR files? You are currently providing about 5% par2 recovery files. In one case (the gen10000esis fileset), using astraweb as my usenet provider, I was unable to recover from checksum mismatch errors -- 67 additional blocks were still required for repair. In this case I was able  to  successfully re-download this fileset from my ISP-provided newsserver (*very slow*) which gets its data from gigagnews. <br><br>It would be good to make the Usenet backup of libgen as robust as possible; increasing the percentage of available PAR2 files would go a long way to achieving this. While this would increase your upload times by 5-10%  it would greatly increase the chances of successfully downloading libgen material without having to use backup servers.<br><br>In any case, you're doing the world a great service. Thank you again!<br><br>@bookwarrior: Of course I also have to thank you and the libgen crew for creating this amazing library. I am sure thousands all over the world think kind thoughts for you and all your efforts.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent"">@123dutch<br><br>I do have one suggestion:<br>Would you consider increasing the percentage of PAR files? You are currently providing about 5% par2 recovery files. In one case (the gen10000esis fileset), using astraweb as my usenet provider, I was unable to recover from checksum mismatch errors -- 67 additional blocks were still required for repair. In this case I was able  to  successfully re-download this fileset from my ISP-provided newsserver (*very slow*) which gets its data from gigagnews. <br><br></div><br><br>Dear ansaron,<br><br>Thanks very much for your reply. It is nice to see that the NZB''s are actually used <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>Some newer NZB''s have been uploaded already, but I will increase par2 to 10%, starting with gen108000essis .<br><br>Kind regards,<br>123dutch</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent"">Dear ansaron,<br><br>Thanks very much for your reply. It is nice to see that the NZB''s are actually used <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>Some newer NZB''s have been uploaded already, but I will increase par2 to 10%, starting with gen108000essis .<br><br>Kind regards,<br>123dutch</div><br><br>Dear 123dutch,<br><br>I'm sure there are many others out there making use of your Usenet uploads too -- and numbers will increase as word about the availability of the libgen archive via Usenet gets out. I do appreciate your decision to increaser the percentage of par2 repair files; Thanks!<br><br>I am now systematically downloading the available archives and came across one unusual error: In downloading the gen22000isis fileset  there were some checksum erros, but these were repairable by the parity files. However, the resulting repaired rar files were then reported as being corrupt. On OS X  the decompression tools refused to extract files, reporting either a corrupted  or passworded archive. I moved the rar files to a windows machine which extracted 998 out of 1000 files, reporting two files as  being corrupt:<br>!   I:\1358885413\gen22000esis.part01.rar: CRC failed in 22000\00b010f787b357e24133a8b976650e67. The file is corrupt<br>!   I:\1358885413\gen22000esis.part04.rar: CRC failed in 22000\650c08e11b676a4dc2f5ede8c8287a4c. The file is corrupt<br><br>I downloaded the archive twice; once using astraweb and once using TweakNews, so I don't think this is a Usenet provider problem (in any case the archive has been successfully repaired so the problem is deeper.) Could you check on this?<br><br>One additional thing: you're splitting up the rar files at the 1GB level; this is (based at least on my experience of Usenet) unusually large. Is there a reason for this? Is there slightly less overhead in uploading a smaller number of files? The reason I ask is that I'm getting more checksum errors than I'm used to seeing. Could it be that the very large size of the rar files is responsible? Would it be a burden to decrease the size of each rar file to, say, 100MB? (My usual Usenet provider astraweb is normally quite reliable -- except when they overeagerly obey DMCA takedown orders! --  so it's a bit worrying.)<br><br>Please do let me know if you want these sort of reports, or if you'd prefer me to shut up  <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile"">.<br><br>In any case, with great gratitude,<br>ansaron</div>",
,,,"<div class=""postbody"">Haha, no please keep the remarks coming <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>1GB is big, but since some archives are 20G I thought we would get so many files... I can make the rar''s smaller if that helps.<br>I wonder if that changes the outcome. I am using giganews so I have no idea if that makes a difference towards other providers. 100MB on the other hand seems so small, since you would get really many files.<br><br>Now I am not verifying the rar. I will from now on.<br>Now I am not adding a recovery archive to the rar. Maybe I should.<br><br>So I would like to hear your opinion on number 2 and 3:<br>1. I raised the par2 to 10% already &lt;-- good for recovering, but the 9TB size gets 5% bigger. (it is a choice to download all par2 files - so this is not a real problem - if you have giganews you will almost never need the par2 files)<br>2. I can add the recovery archive &lt;-- this would increase the chance of recovering (if used well), but also this would increase the total size of the archive. (this increases the total size for everyone)<br>3. making smaller sizes like 100MB parts instead of 1GB parts would really help?<br><br>kind regards,<br>123dutch</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent""><br>So I would like to hear your opinion on number 2 and 3:<br>1. I raised the par2 to 10% already &lt;-- good for recovering, but the 9TB size gets 5% bigger. (it is a choice to download all par2 files - so this is not a real problem - if you have giganews you will almost never need the par2 files)<br>2. I can add the recovery archive &lt;-- this would increase the chance of recovering (if used well), but also this would increase the total size of the archive. (this increases the total size for everyone)<br>3. making smaller sizes like 100MB parts instead of 1GB parts would really help?<br><br>kind regards,<br>123dutch</div><br><br>Good to know your original Usenet provider; as many of the checksum errors are introduced in the mirroring process Giganews might be a good choice for people intent on downloading libgen.  I was having a particularly bad time with astraweb. After I switched to TweakNews the number of corrupted files went down. I will certainly try GigaNews next.<br><br>In response to your points:<br>(1) Yes, it is a choice, and in fact some nzb clients are able to automatically decide if the par2 files are even required. Since some people will be using substandard ISP-provided nntp servers being generous with par files seems to be a very good idea. <br><br>(2) I think we should think in terms of maximizing the longterm robustness of  the Usenet backup of libgen. Since these files are hopefully going to be on Usenet servers for 4+ years (hopefully even longer as Usenet providers keep on extending their retention levels) we should aim at providing files that are are almost guaranteed to flawlessly decompress. Taking that longterm view means that an additional 5-10% downloading time is not a significant demerit. I vote for recovery archives.<br><br>(3) I honestly don't know. I have absolutely no experience of uploading to Usenet. My comments were based only on my extensive Usenet downloading. For now keep things as they are. I'm going to be using your NZBs 24/7 from today. If I encounter too frequent errors I'll let you know and you can try and adjust the RAR  part size down.<br><br>A question:  do you have this set up as some sort of automated cron job or are you manually preparing these uploads?  <br><br>One last comment: you started posting the libgen archive with the ID CPP-gebruiker, then recently switched to CPP-user (I know enough Dutch to know that's the same thing <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""> ).  The last libgen archive that you posted as CPP-gebruiker was gen105000esis which after 2 days is still incomplete, while later archives are complete. Is there possibly a problem with the gen105000esis upload?       <br><br>As always, my thanks and kindest regards,<br>A.</div>",
,,,"<div class=""postbody"">Thanks for your comments again - they are welcomed.<br>I will make the adjustments for next uploads. <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>All is done manually. So I can simply make any adjustments without problem of things not uploaded yet. And if we encounter things that could have been uploader BETTER I will re-upload them.<br>ccp-user vs ccp-gebruiker: I was using my work-connection to upload since it much faster at that moment, and the upload killed my laptop uploading the gen105000eses. Killed the programm I use for uploading also (camelsystempowerpost). I tried some tricks - changing from the dutch to english version of the programm a few times - and it took me 4 days or so to finally find the problem. The user/gebruiker is from me using the english or dutch programm to upload. 105000 is still to be uploaded. I thought at some point that the error might be in the 105000, but it was not.<br>I uploaded the 106, 107 and 108 already. Will continue with the 105 next, using the new settings.<br><br>Please keep reporting so I have a list of things that need to be re-upped or not.<br>Thanks.<br><br>123dutch</div>",
,,,"<div class=""postbody"">123dutch<br><br>have you actually check MD5's before uploading to Usenet? It'll probably take a week, but it's worth doing, from our practice it always discovers surprizes. HDDs are imperfect, possibly 1 byte per billion isn't written correctly etc. Once the files settle in some machine it is desirable to do such a check once.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">123dutch<br><br>have you actually check MD5's before uploading to Usenet? It'll probably take a week, but it's worth doing, from our practice it always discovers surprizes. HDDs are imperfect, possibly 1 byte per billion isn't written correctly etc. Once the files settle in some machine it is desirable to do such a check once.</div><br><br>Good addition.<br>Texas is checking them now.<br>He started with 110.000 and up at my request now, before rarring them.<br>Those are all identical (except for a few files missing  - about 15 files or so are missing from the 9TB because they were infected (or most likely infected) by a virus - so IF you or anyone else sees an archive not containing 1000 but 998 books, then THAT is the reason).<br><br>So far all checks out  - the server we are using is quite fast, so checking md5 will take about 6 minutes per 1000 (88hours) - all will be checked before uploading now. If we encounter an md5 error in the ones uploaded already, then I will re-up.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">123dutch<br><br>have you actually check MD5's before uploading to Usenet? It'll probably take a week, but it's worth doing, from our practice it always discovers surprizes. HDDs are imperfect, possibly 1 byte per billion isn't written correctly etc. Once the files settle in some machine it is desirable to do such a check once.</div><br><br>@Bookwarrior: <br><br>It is really nice that you use the MD5 hash as the filename in libgen -- it makes the checking of a file's integrity self-contained.<br><br>If people are going to check the MD5s of libgen archives they might want to look at md5deep (<a href=""http://md5deep.sourceforge.net/"" class=""postlink"">http://md5deep.sourceforge.net/</a>). This has some very nice features compared to the usual md5sum program. The ones relevant to us are:<br><div class=""quotetitle""><b>Quote:</b></div><div class=""quotecontent"">1. Recursive operation - md5deep is able to recursive examine an entire directory tree. That is, compute the MD5 for every file in a directory and for every file in every subdirectory.<br>2. Comparison mode - md5deep can accept a list of known hashes and compare them to a set of input files. The program can display either those input files that match the list of known hashes or those that do not match.</div></div>",
,,,"<div class=""postbody"">123dutch<br><br>somewhen after you filter out the suspecious infected files, we'd appreciate the listing: in most cases in the past it was false positive, but there are many executables in LG yet (man power problem). Although 1 in a 1000 looks quite wrong, especially if it's in the first 100k. But it's all possible of course.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold"">Report on Libgen Usenet Postings gen0esis--gen25000esis:</span><br><br><span style=""font-weight: bold"">Directory:.......# Files:.......MD5 Mismatches:</span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">    /0        999       0      <br> /1000       1000       0<br> /2000       1000       0<br> /3000       1000       0  <br> /4000       1000       0<br> /5000       1000       0 <br> /6000       1000       0<br> /7000       1000       0<br> /8000       1000       0<br> /9000       1000       0<br>/10000       1000       0<br>/11000       1000       0<br>/12000       1000       0<br>/13000       1000       0<br>/14000       1000       0<br>/15000       1000       0<br>/16000       1000       0<br>/17000       1000       0<br>/18000       1000       0<br>/19000       1000       0<br>/20000       1000       0<br>/21000       1000       0<br>/22000        998       1 (Repost: 1000 files, 0 MD5 Mismatches)<br>/23000       1000       0<br>/24000       1000       0<br>/25000       1000       0<br></div><br>All archives except gen22000esis expanded correctly. <br><br>In gen22000esis the rar archive was reported as being corrupted. 998/1000 files were extracted, with the remaining two being reported as corrupt. In addition md5deep found a hash mismatch for the file 22000/f54a3bce73a27a235960db70c636aded  <br><br>Conclusion: gen22000esis needs to be re-upped.<br><br>I will post reports on 123dutch's Usenet uploads (including MD5 verification) in batches of 25000.<br><br><span style=""font-weight: bold"">EDIT:</span> gen22000esis-repost.nzb is perfect. It decompresses without error and there are no MD5 mismatches.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent"">I will post reports on 123dutch's Usenet uploads (including MD5 verification) in batches of 25000.</div>perfect!!! Thanks a lot!</div>",
,,,"<div class=""postbody"">The 22000 is one of the archives before I verified rar''s... I downloaded this one, extracted, and it gives 2 crc errors.<br>The 22000 will be reupped today, name: gen22000esis-repost  , but ofcourse the nzb will be added to the repository as well.<br><br>Texas is busy verifying all archives for missing books. Also for MD5 mismatches. He will probably finish today or tomorrow and report his findings here.<br><br>Note: today I encountered the first time with creating par2 files that quickpar gave an error creating the files. I did my usually tests and tricks  - and sometimes changing the settings in quickpar could fix it - but the bigger the archive the more difficult to fix. On the net is explained that when you have too much internal memory quickpar can give these kind of errors. The only one that I could not make par files for was 121000 so far (which is a huge archive of 33GB) - not even with another programm. But on my laptop I have only 4GB so I am making it there with no problem (just takes very long)</div>",
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,texas,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,Guest,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,texas,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,silverware,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,texas,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&sid=aaeabbb0ea8145ec7c3ebc56c3963918&start=25&t=6769,texas,,bookwarrior
,,,"<div class=""postbody"">i guess you can just split it into two parts, why to keep the 1-to-1 identity with LG? there is no reason for this. I.e. you can make two files<br>gen121000esis-part1of2<br>gen121000esis-part2of2<br><br>that's pretty much it, if I understood the problem. No animals will be harmed.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">123dutch<br><br>somewhen after you filter out the suspecious infected files, we'd appreciate the listing: in most cases in the past it was false positive, but there are many executables in LG yet (man power problem). Although 1 in a 1000 looks quite wrong, especially if it's in the first 100k. But it's all possible of course.</div><br><br>Hi bw, we finished checking all the folders regarding the number of files in each one, and a virus-check on them.<br><br>We found only 3 virus detected by MS Essentials (all .exe files):<br>118000\fdc00245581b73b84d50cb4ef7a6143d<br>210000\61415ac7cfb96b7fcb967f5532b073e9<br>224000\140d912e60dd2fd88f75b9d7f2c6a653<br><br>In the past, I had some experiences with these ebook compilers that were identified as virus, though there were false positives, and maybe it's the case (better recheck with other antivirus as well).<br><br>About the MD5 checks (thanks ansaron and bw for the idea &amp; checks), we'e already on the folder 250.000, and in next days we hope to finish this as well. In the files already uploaded in Usenet, we found only 5 files with MD5 errors, which are:<br>35000\342ee41dd4c552bb8a14885b3064dbcb<br>35000\4f03f64700ca520a36ce3d7ea6356dfc<br>37000\e950c65a6e4e4df7e3a6d7a5cbe72e13<br>46000\6ef53368dd0404612fc4dbe932dbe21a<br>49000\211d062cd4a5bf38380bb413446df433<br><br>We are thinking if we reupload all these folders again, with the correct files fixed, or just these 5 files to be replaced (as a 'patch', so people download and overwrite our uploaded folders with them). Now we are double checking to avoid any MD5 or unrarring errors in future NZBs  <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br><br>Please keep coming with suggestions, ideas and checking the files, all this helped us to fix a lot of things too <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",
,,,"<div class=""postbody"">yes, that's how it's been happening with us: first upload of LG of about 50k took me 3 almost full attempts, making me iteratively learn what the issues were, just think of this number, it's satanic, if to have no mechanism of file integrity checking. If batches of folders arrive with no errors, it is more a pleasant exception, than a rule. But once you check them after writing onto an HDD, they are going to be more stable, than during the transfer process (including HDD writing). Reading isn't harmful, the rest of the file lifetime will only depend on the hardware lifetime which is the fundamental limit, from our statistics no longer that 1 year per HDD in average when files are accessed randomly, but I can't give a figure how often, possibly at maximum ~10 files/min in average.<br><br>btw, it's not only about HDDs of course, TCP/IP also admits some rate of errors, it does not absolutely guarantee delivery, it has checking mechanisms, but they do not guarantee all 100%.</div>",
,,,"<div class=""postbody"">Dear 123dutch,<br><br>I've downloaded the repost of gen22000esis.<br><br>First, the change to 100MB rar segments has helped: this is the first large (&gt;5GB) libgen archive that I've downloaded that did not require any par files. It also decompressed without error.<br><br>All MD5 hashes are verified too. Outstanding!<br><br>I'll edit my previous report post to point out that the repost of gen22000esis is error free. <br><br>Thanks,<br>A.</div>",
,,,"<div class=""postbody"">Hi 123dutch,<br><br><!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_nzb/gen107000esis.rar"">ftp://libgen.org/repository_nzb/gen107000esis.rar</a><!-- m --> is broken. If I search for the nzb elsewhere it says that it's incomplete: <!-- m --><a class=""postlink"" href=""http://www.nzbsearch.net/search.aspx?q=gen107000esis&amp;st=5"">http://www.nzbsearch.net/search.aspx?q= ... 0esis&amp;st=5</a><!-- m --><br><br>Can you please repost it? Thanks.<br><br>Thank you</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen26000esis–gen50000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br>/26000        1000        0 <br>/27000        1000        0<br>/28000        1000        0<br>/29000        1000        0<br>/30000        1000        0<br>/31000        1000        0<br>/32000        1000        0<br>/33000        1000        0<br>/34000        1000        0<br>/35000        1000        2 (0 with patch)<br>/36000        1000        0<br>/37000        1000        1 (0 with patch)<br>/38000        1000        0<br>/39000        1000        0<br>/40000        1000        0<br>/41000        1000        0<br>/42000        1000        0<br>/43000         999        0<br>/44000        1000        0      <br>/45000        1000        0<br>/46000        1000        1 (0 with patch)<br>/47000         999        0        <br>/48000        1000        0 <br>/49000        1000        1 (0 with patch)       <br>/50000        1000        0<br></div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>All archives except gen43000esis and gen47000esis expanded correctly.  <br><br>In gen43000esis the rar archive was reported as corrupt. 999/1000 files were extracted with the remaining file being reported as corrupt:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">!   F:\gen43000esis\gen43000esis.part3.rar: CRC failed in 43000\6e7c0c60951c7cc6a06ff67dfc6907ec. The file is corrupt<br></div><br>In gen47000esis the rar archive was reported as corrupt. 999/1000 files were extracted with the remaining file being reported as corrupt:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">!   F:\gen47000esis\gen47000esis.part4.rar: CRC failed in 47000\af253377df19e1631cc37ae2600ad74d. The file is corrupt<br></div><br>In both cases my OS X unrar utilities were unable to decompress the rar archives; WinRar was however able to extract all of the uncorrupted files.<br>All of the files that were extracted from these two archives were verified as having the correct MD5 hash.<br><br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>In addition md5deep found MD5 hash mismatches for the files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 does NOT match<br>/35000/8be66d2107a296a63c2f526df184f37d does NOT match<br>/37000/e950c65a6e4e4df7e3a6d7a5cbe72e13 does NOT match<br>/46000/6ef53368dd0404612fc4dbe932dbe21a does NOT match<br>/49000/211d062cd4a5bf38380bb413446df433 does NOT match<br></div><br>(texas and 123dutch already know about the problems with three of these five files. Their MD5 mismatch results for /35000 do not correlate with mine. Please check.)<br><br><br><span style=""font-weight: bold"">Suggestions: <br></span>(1) Create a `patch' file to replace the files with MD5 hash mismatches (re-uploading whole archives is overkill).<br>(2) Upload corrected versions of gen43000esis and gen47000esis since the rar archives are rejected entirely by many decompression utilities.<br><br><br>As always, thanks to 123dutch and texas for all their work in getting Library Genesis onto Usenet.<br><br><span style=""font-weight: bold"">Edit:</span> <br>(1) Remark on MD5 mismatches in /35000 files.<br>(2) (9 February 2013): gen PATCH esis corrects the mismatch errors in /35000, /37000, 46000, 49000. In the case of /35000 you should also delete the existing two files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 <br>/35000/8be66d2107a296a63c2f526df184f37d</div></div>",
,,,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent""><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>In addition md5deep found MD5 hash mismatches for the files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 does NOT match<br>/35000/8be66d2107a296a63c2f526df184f37d does NOT match<br>/37000/e950c65a6e4e4df7e3a6d7a5cbe72e13 does NOT match<br>/46000/6ef53368dd0404612fc4dbe932dbe21a does NOT match<br>/49000/211d062cd4a5bf38380bb413446df433 does NOT match<br></div><br>(texas and 123dutch already know about the problems with three of these five files. Their MD5 mismatch results for /35000 do not correlate with mine. Please check.)<br><br></div><br><br>Thanks again, ansaron, for your great work checking the files, about the corrupted RAR Archives, I believe dutch will reupload them soon. <br><br>Regarding the MD5, the best way indeed will be to upload just a patch with the 5 files, however, regarding the /35000 folder (with the 2 problematic files), I believe they were replaced in the next versions of the LG database. <br><br>For example, the file 4e04cd97600452a602cae12f28ab43c1 isn't found in libgen.org if we perform a MD5 search. But, if we hash the actual file, the MD5 will be 4f03f64700ca520a36ce3d7ea6356dfc (so there's a mismatch as you correctly observed), and now this file is in LG:<br><br><!-- m --><a class=""postlink"" href=""http://libgen.org/book/index.php?md5=4F03F64700CA520A36CE3D7EA6356DFC"">http://libgen.org/book/index.php?md5=4F ... 7EA6356DFC</a><!-- m --><br><br>The same with the 8be66d2107a296a63c2f526df184f37d, whose MD5 is 342ee41dd4c552bb8a14885b3064dbcb, and we find <!-- m --><a class=""postlink"" href=""http://libgen.org/book/index.php?md5=342ee41dd4c552bb8a14885b3064dbcb"">http://libgen.org/book/index.php?md5=34 ... 5b3064dbcb</a><!-- m --><br><br>So, I believe that when the torrent was started, these 2 files had errors and only after, the database was fixed, but the torrents kept the original files (please bw and Bill, correct me if I'm wrong). <br><br>The other 3 files indeed had erros in our folders. I uploaded the patch <a href=""http://www.filefactory.com/file/2m9mhyaptq0p/n/LG_Patch_rar"" class=""postlink"">here</a>, but soon dutch will put this rar in Usenet.<br><br>Thanks again, and please keep bringing these infos, and any suggestions.</div>",
,,,"<div class=""postbody"">texas, thank you very much for the reply, which is very reassuring. I was puzzled and quite concerned by this anomaly. I am happy though that the checks we have in place are picking up these problems –- the underlying LibGen filing system is well thought out and is doing its job.<br><br>The patch file seems perfect. There are now no MD5 mismatch errors in the corrected directories. Once the patch file is uploaded to Usenet I will edit the previous report, including a note that the two anomalous files in /35000 need to be deleted.<br><br>The error rate is very low. As bookwarrior pointed out we should expect some errors in the early stages. I think once we get past /104000 the combination of MD5 checks + RAR repair files + smaller RAR segments will force the error rate down to zero (I hope!). <br><br>Best wishes,<br>A.</div>",
,,,"<div class=""postbody"">I believe there must be some optimal length of the crc code for a particular error correction/recovery algorithm. Possibly RAR has some open numbers or some easy tests can be made with the archiver itself (pack with and without recovery option, compare the sizes) to figure out what fraction of the data the recovery code should optimally take. With a binary/hex editor one can modify 1, 2, ..., N bits to see how the algorithm reacts. The numbers on the reliability of HDD writing and TCP/IP can be found, this will give a firm figure (I someone is heroic enough to get thru) what the reasonable amount of recovery information should be per unit of volume. I'm just speculating...</div>",
,,,"<div class=""postbody"">Re: error rates - see this W#kipedia article <a href=""http://en.wikipedia.org/wiki/ZFS#Error_Rates_in_Harddisks"" class=""postlink"">http://en.wikipedia.org/wiki/ZFS#Error_Rates_in_Harddisks</a></div>",
,,,"<div class=""postbody"">there are several broken nzb files on <!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_nzb/"">ftp://libgen.org/repository_nzb/</a><!-- m --> nzb's 118-121 that is - most files come out broken. If I get the nzb from another source like <!-- m --><a class=""postlink"" href=""http://www.nzbsearch.net/search.aspx?q=gen121000esis&amp;st=5"">http://www.nzbsearch.net/search.aspx?q= ... 0esis&amp;st=5</a><!-- m --> it works fine. Thank you.</div>",
,,,"<div class=""postbody"">this tuesday the upload continues again: 43000, 47000, 107000 and contuing with 131000 and up.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen51000esis–gen75000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br>/51000        1000        0 <br>/52000        1000        0<br>/53000        1015?       0<br>/54000        1000        0<br>/55000        1001?       0<br>/56000        1000        0<br>/57000         999        0<br>/58000        1000        0<br>/59000        1000        0<br>/60000        1000        0<br>/61000        1000        0<br>/62000        1000        0<br>/63000        1000        0<br>/64000        1000        0<br>/65000        1000        0<br>/66000        1000        0<br>/67000        1000        0<br>/68000         999        0<br>/69000        1000        0     <br>/70000         997        0<br>/71000         996        0<br>/72000         998        0       <br>/73000         997        0<br>/74000        1000        0        <br>/75000         994        0<br></div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>NO CORRUPTED ARCHIVES. All archives expanded correctly.  <br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found NO MD5 MISMATCHES.<br><br><span style=""font-weight: bold"">File Numbers:<br></span>Could I have confirmation of the correct/expected number of files in each directory? <br><br><span style=""font-weight: bold"">Remark on Patches:<br></span>gen PATCH esis corrects previous MD5 mismatches in /35000, /37000/, /46000 and /49000. A note about this will be added to the previous report.<br><br>As always, thanks to 123dutch and texas for their work. Your efforts are deeply appreciated. The error rate for this batch was zero. Outstanding!</div>",
,,,"<div class=""postbody"">any reason why 81000 and 82000 are missing? they were skipped and went right into 83000</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">reopert wrote:</div><div class=""quotecontent"">any reason why 81000 and 82000 are missing? they were skipped and went right into 83000</div><br><br>They do not exist/are empty.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent""><br><span style=""font-weight: bold"">File Numbers:<br></span>Could I have confirmation of the correct/expected number of files in each directory? <br></div><br>Here is the list of the number of files in each directory, according to the torrent files. In our NZBs we just removed 3 files, detected as virus by MS Essentials (.exe files), so these 3 folders will have 999 files in the NZBs. The list is:<br><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">#Files  Folder<br>----   -----<br>54    /83000<br>257   /77000<br>290   /80000<br>318   /865000<br>351   /866000<br>515   /76000<br>913   /212000<br>917   /213000<br>985   /52000<br>994   /75000<br>996   /200000<br>996   /237000<br>996   /71000<br>997   /107000<br>997   /248000<br>997   /308000<br>997   /70000<br>997   /73000<br>997   /869000<br>998   /108000<br>998   /124000<br>998   /201000<br>998   /202000<br>998   /225000<br>998   /72000<br>998   /721000<br>998   /844000<br>999   /0<br>999   /104000<br>999   /114000<br>999   /115000<br>999   /118000 * virus (.exe - /118000/fdc00245581b73b84d50cb4ef7a6143d)<br>999   /203000<br>999   /210000 * virus (.exe - /210000/61415ac7cfb96b7fcb967f5532b073e9)<br>999   /223000<br>999   /224000 * virus (.exe - /224000/140d912e60dd2fd88f75b9d7f2c6a653)<br>999   /226000<br>999   /239000<br>999   /244000<br>999   /251000<br>999   /294000<br>999   /307000<br>999   /327000<br>999   /57000<br>999   /786000<br>999   /867000<br>1001   /55000<br>1015   /53000<br><br>All other folders = 1000 files</div><br><br>Some folders indeed have much less than 1000 files. Maybe BW or Bill can confirm or correct these numbers, if possible, so our NZBs can be up-to-date or eventually corrected to the actual LG now.<br><br>Thanks again for checking the NZBs! Your help and of course BW and Bill_G efforts are also outstanding, no doubt at all!  <img src=""./images/smilies/icon_e_biggrin.gif"" alt="":D"" title=""Very Happy""><br><br>Cheers!<br>Texas</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen76000esis–gen100000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br> /76000          515        0 <br> /77000          257        0<br> /78000         1000        0<br> /79000         1000        0<br> /80000          290        0<br> /81000         DOES NOT EXIST<br> /82000         DOES NOT EXIST<br> /83000           54        0<br> /84000         1000        0<br> /85000         1000        0<br> /86000         1000        0<br> /87000         1000        0<br> /88000         1000        0<br> /89000         1000        0<br> /90000         1000        0<br> /91000         1000        0<br> /92000         1000        0<br> /93000         1000        0<br> /94000         1000        0     <br> /95000         1000        0<br> /96000         1000        0<br> /97000         1000        0       <br> /98000         1000        0<br> /99000         1000        0        <br>/100000         1000        0</div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>There were NO CORRUPTED ARCHIVES. The gen78000esis archive <span style=""font-style: italic"">was</span> reported as being corrupt by MacPAR deluxe's built-in RAR extractor. However all 1000 files were successfully extracted by other RAR utilities (on both OS X and Windows). WinRAR confirms all files in the archive as being uncorrupted. All MD5 hashes for extracted files are OK. I have no explanation for this (beyond that most OS X decompression utilities seem more fragile than their Windows counterparts). So, if you have trouble expanding the gen78000esis RAR archives try a different decompression utility.    <br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found NO MD5 MISMATCHES.<br><br><br>The error rate for this batch was again zero. We are winning  <img src=""./images/smilies/icon_cool.gif"" alt=""8-)"" title=""Cool""></div>",
,,,"<div class=""postbody"">gen155000esis is missing at least one .rar file. It ends with part120, but it's incomplete - at least part 121 is missing. Getting only 992 files.</div>",
,,,"<div class=""postbody"">I will re-up gen155000esis monday <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""> Thanks for the reply <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",
,,,"<div class=""postbody"">our gen.lib.rus.ec's hoster seems to have sold his project (lib.rus.ec) or is close to this. No one knows what the fate of gen.lib.rus.ec is. So, the backups of any sort become even more important now.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">our gen.lib.rus.ec's hoster seems to have sold his project (lib.rus.ec) or is close to this. No one knows what the fate of gen.lib.rus.ec is. So, the backups of any sort become even more important now.</div><br><br>LG on usenet will continue (I am sick for a few weeks, hence the somewhat slower uploads). <br>But no matter what, we will keep on pushing the files to Usenet.</div>",
,,,"<div class=""postbody"">@123dutch<br><br>Someone else pointed this out before, but it's only just bitten me: gen118000esis -- gen121000esis are all incomplete (no matter which  NZBs you use). That is: the uploads are all missing too many blocks for repair to be possible (in many cases the PAR files are damaged too). <br><br>Would it be possible for you to either upload additional PAR files or else maybe totally re-upload these four archives again? At the moment they are damaged beyond repair.<br><br>Many thanks (and I hope you get better soon!).<br><br><span style=""font-weight: bold"">UPDATE:</span> I'm now trying to use the recovery volumes in the RAR volumes to first (partially) repair the RAR archives and then use the PAR files to complete the recovery of the archives. I seem to have succeeded in doing this for gen118000esis. It's taking some time so my next report will probably be delayed.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">How to use RAR Recovery Records. Part 1<br></span></span><br>From gen105000esis onwards 123dutch has incorporated recovery records into the RAR archives he is posting on Usenet. This was new to me, so I thought I'd write a short How-To on using RAR recovery records. <br><br>I'll illustrate using two examples from 123dutch's Usenet upload of Library Genesis: gen126000esis and gen118000esis<br><br>We'll start with gen126000esis as this is more straightforward. After downloading the RAR volumes and repairing with PAR2 files, the standard decompression utilities reported that the archive was damaged. <br><br><span style=""font-weight: bold"">Windows</span><br>If you are using WinRAR on a Windows machine then open any of the gen12600esis RAR volumes (say gen126000esis.part001.rar) in WinRAR and select ""test"". <br>After a while WinRAR will identify one volume (gen126000esis.part005.rar) as containing a file with a CRC error.<br>Open gen126000esis.part005.rar with WinRAR and select ""repair"". <br>After a while WinRAR will produce a file: fixed.gen126000esis.part005.rar. <br>Delete the original file gen126000esis.part005.rar and rename fixed.gen126000esis.part005.rar to gen126000esis.part005.rar<br><br>gen126000esis should now decompress without any problems.<br><br><span style=""font-weight: bold"">*nix<br></span>If you are using a *nix variant (linux, OS X, ...) then you will need to use the command-line version of RAR (available from:<a href=""http://www.rarlab.com/download.htm"" class=""postlink"">http://www.rarlab.com/download.htm</a>).<br><br>We start by testing the RAR volumes to identify the volume(s) that requires repair. Change to the directory containing the rar executable and in a terminal window type:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">./rar t '/path/to/rar/volumes/*'</div><br>You should get output that reads:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">RAR 4.20   Copyright (c) 1993-2012 Alexander Roshal   9 Jun 2012<br>Trial version             Type RAR -? for help<br><br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part001.rar<br><br>Testing     126000/00528e2366972d6c027fb4a15e4b18cf                   OK <br>Testing     126000/0093fe6589d2a286206f9d181f9b6d4b                   OK <br>Testing     126000/00c1b48fda5f2f0072710c19a6f2a4c1                   OK <br>Testing     126000/00e61e1d50e9f106cca8e37d093fcaed                   OK<br>...<br>...<br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part005.rar<br><br>...         126000/0792160377d05abb306e2bac7cc43c5f                   OK <br>Testing     126000/079e0eb1cdfbc062e75758ccef635a61                    2%<br>126000/079e0eb1cdfbc062e75758ccef635a61 - CRC failed<br>Testing     126000/07d10822167b765b0031b5975b6a1ea2                   OK <br>Testing     126000/07dbc84267f12801736dbafcf791661f                   OK <br>Testing     126000/0807b545d7f25c85cc9dc0068ec816be                   OK <br>Testing     126000/08a244119244b8efe3c6f04b45ab5527                   OK <br>Testing     126000/08be1615b9af3c644a222012e775d9e0                   OK <br>Testing     126000/0910705a9d46846f5efdcac1e1d1b146                   OK <br>Testing     126000/092a34a90c230bc814dd3163398ca4e7                    2%<br>...<br>...<br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part086.rar<br><br>...         126000/fb931f2d8b7c464ed78c6f1e51cca6a7                   OK <br>Testing     126000/fbb9b0426e09282efcb2164f04a253dc                   OK <br>Testing     126000/fbdbcbe36bb3e00e805ee3f9470be247                   OK <br>Testing     126000/fc2d16b77ef81815aaf9aee3a154ced8                   OK <br>...<br>...<br>Total errors: 1<br></div><br>So in this case there is a single RAR volume (part005) containing a corrupted file.  We now repair that RAR volume:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">./rar r /path/to/rar/volumes/gen126000esis.part005.rar</div><br>You should get output:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">RAR 4.20   Copyright (c) 1993-2012 Alexander Roshal   9 Jun 2012<br>Trial version             Type RAR -? for help<br><br>Building fixed.gen126000esis.part005.rar<br>Scanning...<br>Data recovery record found 100%<br>Sector 36051 (offsets 119A600...119A800) damaged - data recovered<br>Sector 36052 (offsets 119A800...119AA00) damaged - data recovered<br>Sector 36053 (offsets 119AA00...119AC00) damaged - data recovered<br>Sector 36054 (offsets 119AC00...119AE00) damaged - data recovered<br>Sector 36055 (offsets 119AE00...119B000) damaged - data recovered<br>Done</div><br>and you end up with a file fixed.gen126000esis.part005.rar (in the same directory where the rar executable lives). Replace the original part005 volume with the fixed volume.  The  gen126000esis archive should now decompress without error.</div>",
,,,"<div class=""postbody"">thanks, I believe it will be helpful to others.</div>",
,,,"<div class=""postbody"">@ansaron: Thanks a lot for your tutorial, and for you testings as well!<br><br>@BW: Today and in next days were uploading the books from folders 200k-300k, and we noticed that there's the fllowing batch file which deletes some books with broken links (MD5 mismatches?):<br><!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_torrent/del%20broken%20files.bat"">ftp://libgen.org/repository_torrent/del ... 0files.bat</a><!-- m --><br><br>So, it's better to delete the books from these folders, by running the .bat, and after they are removed, upload in Usenet the folders without these books already?<br><br>Thanks you all again for your great work!</div>",
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=50&t=6769,ansaron,,bookwarrior
,,,"<div class=""postbody"">if you have a chance, store a list of removed files (with folder names) so that by the end of your checks we can have a list of troubled files and do something about it in future: checking and probably (can't say it'll really happen) rebuilding the torrents will become easy.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Using RAR Recovery Records. Part 2</span></span><br><br>gen118000esis is a more complex example. Here we find that there are not enough PAR2 recovery blocks to restore the RAR archives. However, using the recovery records contained in the RAR files we can gain enough blocks to make the archives restorable.<br><br>First check which RAR volumes are damaged. Either use your PAR program or else check the NZB listing to see which volumes were uploaded incomplete. For instance, binsearch shows that the first couple of damaged RAR volumes are:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">   <br>[44/93] - ""gen118000esis.part043.rar"" yEnc (1/547) incomplete (546 / 547)<br>[43/93] - ""gen118000esis.part042.rar"" yEnc (1/547) incomplete (545 / 547)         <br>[42/93] - ""gen118000esis.part041.rar"" yEnc (1/547) incomplete (546 / 547)<br>[41/93] - ""gen118000esis.part040.rar"" yEnc (1/547) incomplete (545 / 547)<br>[39/93] - ""gen118000esis.part038.rar"" yEnc (1/547) incomplete (546 / 547)<br>[38/93] - ""gen118000esis.part037.rar"" yEnc (1/547) incomplete (545 / 547)<br>[37/93] - ""gen118000esis.part036.rar"" yEnc (1/547) incomplete (546 / 547)</div><br>We now repeat the repair procedure outlined in Part 1 of this tutorial. Using either  the Windows GUI (for Windows machines) or the command-line tools (in the case of Unices) we attempt repair of the damaged  RAR volumes. In the case where only one or two blocks are missing the repair should be total (i.e., the RAR volume will be completely restored). In some cases only partial restoration of RAR volumes is possible. On the command-line versions you might encounter comething like:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">...<br>Sector 35248 (offsets 1136000...1136200) damaged - data recovered<br>Sector 35249 (offsets 1136200...1136400) damaged - data recovered<br>Sector 90000 (offsets 2BF2000...2C31C00) damaged - cannot recover data<br>Reconstruct archive structure ? [Y]es, [N]o </div><br>Answer No (or just press return). <br><br>You need to repeat this procedure and repair as many RAR volumes as is required: that is, until you have recovered enough blocks to allow the PAR program to restore the entire set of RAR  volumes. In the case of gen118000esis it is possible to do this. This can be very time consuming and you may well want to use scripting tools to automate some of the work. (It's a pain even then! <img src=""./images/smilies/icon_e_sad.gif"" alt="":("" title=""Sad"">)<br><br>In the case of, for instance, gen119000esis, it is <span style=""font-weight: bold"">not</span> possible to  restore the RAR volumes completely -- the PAR program reporting that even after RAR repair an additional 36 blocks are required for restoration.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen101000esis–gen125000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:          MD5 Mismatches:<br>/101000         1000/1000        0<br>/102000         1000/1000        0<br>/103000          999/1000        0<br>/104000          999/999         0<br>/105000         1000/1000        0 <br>/106000          999/1000        0<br>/107000          997/997         0<br>/108000          998/998         0<br>/109000         1000/1000        0<br>/110000         1000/1000        0<br>/111000         1000/1000        0<br>/112000         1000/1000        0<br>/113000         1000/1000        0<br>/114000          999/999 **      0<br>/115000          999/999 **      0<br>/116000         1000/1000        0<br>/117000         1000/1000        0<br>/118000          999/999 **      0<br>/119000        NOT RESTORABLE    - UPDATE: Repost numbers: 1000/1000     0<br>/120000        NOT RESTORABLE    - UPDATE: Repost numbers: 1000/1000     0<br>/121000        NOT RESTORABLE    - UPDATE: Repost numbers: 1000/1000     3<br>/122000         1000/1000**      0<br>/123000         1000/1000**      3<br>/124000          998/998         0<br>/125000         1000/1000        0<br></div><br>** IMPORTANT: These numbers assume you have repaired corrupted RAR volumes using RAR Recovery Records. (From gen108000esis? onwards 123dutch has incorporated RAR Recovery Records into the RAR files. This enables us to restore corrupted RAR volumes. I previously posted a short tutorial on how to restore corrupted RAR files (""How to use RAR Recovery Records. Part 1""). Read that post before proceeding.)<br><br><br><span style=""font-weight: bold"">Corrupted RAR archives with no recovery records: Restoration impossible<br></span>gen103000esis was reported as being corrupt. WinRAR (Windows) and The Unarchiver (OS X) were able to extract 999/1000 files, with the one remaining file being reported as corrupt:<br>In archive gen103000esis.part003.rar: Problem with item: 103000/3bfdba969d960893ae4639315020884f<br>There are <span style=""font-weight: bold"">no</span> recovery records in the gen103000esis RAR volumes, so recovery is not possible.<br><br>gen106000esis was reported as being corrupt.  999/1000 files  extracted, one remaining file reported as corrupt:<br>In archive gen106000esis.part006.rar: Problem with item: 106000/52ce7fc76fd87d89555e6f549c900019<br>There are <span style=""font-weight: bold"">no</span> recovery records in the gen106000esis RAR volumes, so recovery is not possible.<br><br><span style=""font-weight: bold"">Corrupted RAR archives with recovery records: Restoration possible<br></span>gen114000esis was reported as being corrupt: <br>Archive gen114000esis.part006.rar contains a corrupted file: 114000/0dec121e071284e76c38b08e82185889 - CRC failed<br><br>You must repair the RAR volume: <br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">gen114000esis.part006.rar<br></div>After repair all files in gen114000esis can be extracted without error.<br><br>gen115000esis was reported as being corrupt: <br>In archive gen115000esis.part001.rar: 115000/000502b337f3eecf034b4b202ec0250b - CRC failed<br>In archive gen115000esis.part076.rar: 115000/a5198a14bbf3ee36af94799cc35bbeb8 - CRC failed<br>You must therefore repair the RAR volumes:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">gen115000esis.part001.rar<br>gen115000esis.part076.rar</div> After these repairs all files in gen115000esis can be extracted without error.<br><br>gen122000esis was reported as being corrupt: <br>In archive gen122000esis.part043.rar incomplete<br>In archive gen122000esis.part094.rar: 122000/adab7cadd386d4b0e93f912a14c38573 <br>In archive gen122000esis.part096.rar: 122000/adab7cadd386d4b0e93f912a14c38573 - CRC failed<br>In archive gen122000esis.part112.rar: 122000/de554921c8505edef566e6561aa8cbab - CRC failed<br><br>You must therefore repair the RAR volumes:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">gen122000esis.part043.rar<br>gen122000esis.part094.rar<br>gen122000esis.part096.rar <br>gen122000esis.part112.rar</div> After these repairs all files in gen122000esis can be extracted without error.<br><br>gen1233000esis was reported as being corrupt. <br>In archive gen123000esis.part22.rar: 123000/39b1897b37d7c711fe7704b39bc0c8e7 - CRC failed<br><br>You must repair the RAR volume:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">gen123000esis.part22.rar<br></div>After repair all files in gen123000esis can be extracted without error.<br><br>The repair of gen118000esis is possible through a combination of PAR2 files and RAR recovery records. See my previous post ""How to use RAR Recovery Records. Part 2"" for details.<br><br>The repair of gen119000esis, gen120000esis and gen121000esis is not possible even with RAR recovery records. <br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found the following MD5 mismatches:.<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/123000/69ad346e5e5363c14b44d230551ef724 does NOT match<br>/123000/8408cd8833098b5f094125b5fc5162b1 does NOT match<br>/123000/fe0d8c15cadd5ad45baa81e5d41aeeb2 does NOT match<br></div><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Actual Hash:                      Filename:<br>324ae8f3ad2e6af911f90a9af5940bc1  /123000/69ad346e5e5363c14b44d230551ef724<br>3ffe60684c22cf24ea3ef8ac890a1702  /123000/8408cd8833098b5f094125b5fc5162b1<br>54377cb07a1f8835c8465ac6f8ee09ae  /123000/fe0d8c15cadd5ad45baa81e5d41aeeb2<br></div><br><span style=""font-weight: bold"">Update on Reposted Archives:</span><br>123dutch has reposted the archives gen118000esis--gen121000esis, which were previously not restorable. These now unpack perfectly.<br><br>In the case of gen121000esis we have the following MD5 mismatches:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/121000/45def3c4de61b84ffb82422ed99ca7d1 does NOT match<br>/121000/49c4f58894b80930c3bd1b93ff4f2cde does NOT match<br>/121000/7a9c067ed09046e66a59f0feb2d68feb does NOT match</div><br><span style=""font-weight: bold"">Suggestions:<br></span>(1) Repost the gen103000esis and gen106000esis archives.<br>(2) Either completely repost the archives  gen118000esis, gen119000esis, gen120000esis and gen121000esis, or else post enough additional PAR files to enable recovery of the RAR volumes. (In principle it is possible to restore gen118000esis using RAR recovery records, but this is very tedious.) <br>(3) The presence of RAR recovery records means that you do not need to repost  gen114000esis, gen115000esis, gen122000esis, gen123000esis. While this is excellent, it seems worrying that there *any* CRC errors in the RAR volumes. Are you testing the integrity of the RAR volumes before producing the PAR files? What tools are you using? Maybe it is the very large size of the archives that is the problem?   <br>(4) Check on the 3 MD5 mismatches in gen123000esis and if required post a a patch for these files?<br><br>123dutch and texas: thank you again for all the work that you're putting into this!<br><br><span style=""font-weight: bold"">UPDATES:</span> <br>(1) Comment on MD5 mismatches in gen123000esis.<br>(2) 8 April 2013: Updated report, taking into account reposts of 119000-121000.</div>",
,,,"<div class=""postbody"">ansaron<br><br>yeah, your checks quantitatively look like when we were uploading LG. I hope this will help people understand how much easier it is to use torrents which automatically take care of the integrity and work effectively as URLs. That's a big job in such collections, if the technology itself does not offer ways to check the integrity.</div>",
,,,"<div class=""postbody"">bookwarrior,<br><br>Yeah, it is more labour intensive than torrenting. I'm glad to know though that the error rate is not excessive.<br><br>In principle the presence of PAR files should mean that the integrity of the RAR archive is assured (assuming not too many blocks are missing). <br><br>The problem (as fas as I can see)  is that the RAR programmes are not packing the files with complete integrity. <br><br>In fact I do not completely trust the available RAR tools. In some cases the test mechanism gives false positives for damaged volumes  -- and then misses some <span style=""font-style: italic"">genuinely</span> damaged volumes. <br><br>I think though that this last batch was an anomaly. Subsequent archives seem to be undamaged and do not require RAR recovery record repair.  I worry too much maybe? I would however prefer to be overly concerned and so  be assured of an accurate mirror of libgen.</div>",
,,,"<div class=""postbody"">in general, RAR is very reliable, but I guess such volumes are a challenge to any tool or protocol. I can easily imagine that RAR has some sort of piece-wise hashing, like chunks in torrents, but I don't know how RAR controls the size of such chunks. Of course, any error correction code can only offer a probability of successful correction, never exact 100%. This is due to the cryptographical nature of these codes, the highly asymmetric encoding/decoding: encoding gives you a hash, but decoding a hash is practically impossible, but theoretically it is. In other words this correction is only statistical: when the total amount of data isn't very large (I don't know how much), it is probably more effective than in the case of terabytes. So, if no other errors during the computation and transfer, it can be that the limitation comes from the RAR mechanism of chosing the optimal chunk size and from the codes/hashes themselves. E.g., if RAR mechanism was somehow optimized till 1 GB and the chunk size was fixed (I'm not aware of the implementation details), the probability of having a failing chunk would be dramatically higher for a data block of 10 TB, and the same residual (after correction) error rate will obviously increase by 4 orders of magnitude.</div>",
,,,"<div class=""postbody"">gen220000esis and gen221000esis are beyond recovery - neither par nor rar's recovery.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen126000esis–gen150000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:        MD5 Mismatches:<br>/126000       1000/1000         4**<br>/127000       1000/1000         2<br>/128000       1000/1000         0<br>/129000       1000/1000         0<br>/130000       1000/1000         0<br>/131000       1000/1000         0<br>/132000       1000/1000         0<br>/133000       1000/1000         0<br>/134000       1000/1000         0<br>/135000       1000/1000         0<br>/136000       1000/1000         0<br>/137000       1000/1000         1<br>/138000       1000/1000         0<br>/139000       1000/1000         0<br>/140000       1000/1000         0<br>/141000       1000/1000         0<br>/142000       1000/1000         0<br>/143000       1000/1000         0<br>/144000       1000/1000         0<br>/145000       1000/1000         0<br>/146000       1000/1000         0<br>/147000       1000/1000         0<br>/148000       1000/1000         0<br>/149000       1000/1000         0<br>/150000       1000/1000         0</div><br>** IMPORTANT: These numbers assume you have repaired corrupted RAR volumes using RAR Recovery Records. See my previous how-to posts on using RAR recovery records.<br><br><span style=""font-weight: bold"">Corrupted Archives Recoverable using RAR Recovery Records:<br></span>After PAR repair gen126000esis was reported as being corrupt. gen126000esis.part005.rar was identified as containing a file with a CRC error; it was restored using the built-in RAR recovery records. The archive was then decompressed without error.<br><br><span style=""font-weight: bold"">MD5 Mismatches:</span><br>md5deep found the following MD5 mismatches:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/126000/0792160377d05abb306e2bac7cc43c5f does NOT match<br>/126000/0e27c72eb60f7aec3fce453c8713d3e1 does NOT match<br>/126000/22926da9787c71ac3249275c6ea851c8 does NOT match<br>/126000/2961ab1530cb3d638978461f1483bbcb does NOT match<br>/127000/2e239bb1985aaa4c4a1d1b3d23b5de60 does NOT match<br>/127000/448227d605421594fb0ebb22d74df89d does NOT match<br>/137000/e39f8c3230ac97e4a07fd452815b07eb does NOT match</div><br><span style=""font-weight: bold"">Suggestions and Notes:<br></span>(1) Check the seven files with MD5 mismatches and upload a patch for these files. <br>(2) The error rate on this batch is way down compared to the previous batch (which was most probably an aberration).</div>",
,,,"<div class=""postbody"">A lot of people in the newsgroups are confused about these massive uploads. They can't figure out what those are. Perhaps tell them what it is and where to get the names of the md5 files?<br><br>I'm just concerned that they could somehow report this activity as spam and and if enough do usenet feed providers would start filtering it out? <br><br>Perhaps I have no reason to be concerned and nothing special needs to be done.</div>",
,,,"<div class=""postbody"">Dear reopert,<br><br>Can you tell me who the people are that might have questions or how I can get in touch with them?<br>May be here - may be in a pm if privacy is needed.<br><br>I don't see it happening that providers would filter out the uploads  - and with nzb's there is no dependency of availability of search engines.<br>Also the uploads are not that massive I think. In Holland usenet is the primary way to share files and I am uploading less than 0.01% in TB's of what is uploaded daily from Holland. But to be on the safe side, if explaining to the worried helps, I can get in touch with them.</div>",
,,,"<div class=""postbody"">In the last 10 messages or so there were a few requests for re-uploads. Can anyone be so kind in gathering them in 1 reply here so i know what to reupload? The reupload is planned for next tuesday. In the meantime I will continue the normal uploads. NZB's will be added later today, but if you search on the several indexes you see that the last uploaded now is gen253000esis. Continuing tomorrow...</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent"">In the last 10 messages or so there were a few requests for re-uploads. Can anyone be so kind in gathering them in 1 reply here so i know what to reupload? The reupload is planned for next tuesday.</div><br><span style=""font-weight: bold"">Requests for reposts:</span><br>gen103000esis  (ansaron's request)<br>gen106000esis  (ansaron's request)<br>gen118000esis  (ansaron's request)<br>gen119000esis  (ansaron's request) <br>gen120000esis  (ansaron's request)<br>gen121000esis  (ansaron's request)<br>gen220000esis  (reopert's request)<br><br>I think that's all? Other repost requests have already been re-uploaded.<br><br><span style=""font-weight: bold"">Edit:</span> (08/03/2013) Removed request for 221000 -- reopert reports it's OK.</div>",
,,,"<div class=""postbody"">123dutch, I saw those in alt.binaries.e-books - people simply reply to your posts in the group and keep wondering what that is, by now they get a standard recommendation for others in the group to put you into the ignore rules. So if you simply check for Re: to your posts you will see what I mean.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">reopert wrote:</div><div class=""quotecontent"">123dutch, I saw those in alt.binaries.e-books - people simply reply to your posts in the group and keep wondering what that is, by now they get a standard recommendation for others in the group to put you into the ignore rules. So if you simply check for Re: to your posts you will see what I mean.</div><br><br>Now I am over-asked... I have no idea how to do that.<br>I never replied on usenet to questions... so I don;t even see them.<br>So how do I do that? In binsearch I do not see these questions...</div>",
,,,"<div class=""postbody"">you can probably go about it in 2 ways <br><br>1. post a special file with explanation of what all the other files are as you'd be sending any other files to those groups<br>2. use a usenet reader app, subscribe to the groups you post the binaries to (e.g. a.b.e-books) and you will see posts replying to your uploads, then you can reply to them<br><br>probably #1 is the easiest to go with and will benefit more people in the long run, rather than hunting for those questions and wasting time replying to them. The best idea I can think of is to bundle an extra file, say .nfo with each volume that will explain what's in the archive and if you could automate the generation of such you could probably even list the actual books in it. if it's too much to ask then a standard explanation .nfo file which would be the same for each volume, you could explain how to look up the md5 hash to find out the name of the book.<br><br>so you'd post something like:<br>gen233000esis.nfo<br>gen233000esis.par2<br>...<br>gen233000esis.part01.rar<br>...<br><br>if you can use perl i can write a quick script for you that will generate the listing for each volume. I'm on linux, so if you're not on unix-based system you may need to tweak it to work. let me know.<br><br>does it make sense?</div>",
,,,"<div class=""postbody"">Thanks <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br>I will add a small NFO now with each archive. I have no idea how to add the contents of each archive, so the nfo contains generic info.<br>I can ask Texas later if he can figure out how to get the 1000 books that belong to an archive - he is our smart guy, I am only a freak uploader.</div>",
,,,"<div class=""postbody"">As i said, i can write a perl script for you that will do it, you will run it like:<br><br>./create-nfo.pl 233000<br><br>233000 being the folder with 1000 books. Though again I can only make one that will work on a unix-based system, if it's windows it may need some tweaking.<br><br>but as you said adding just a generic info file is already an excellent start.<br><br>Thank you.</div>",
,,,"<div class=""postbody"">ansaron, I managed to re-download gen221000esis and correct the problems. for gen220000esis I haven't tried re-downloading - I used a torrent to fill in the holes. So perhaps someone else can test those 2 and save re-uploading if they are ok. I wonder whether my ISP sometimes is slow in getting the complete feed and that's why i get too many incomplete archives.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">reopert wrote:</div><div class=""quotecontent"">ansaron, I managed to re-download gen221000esis and correct the problems. for gen220000esis I haven't tried re-downloading - I used a torrent to fill in the holes. So perhaps someone else can test those 2 and save re-uploading if they are ok. I wonder whether my ISP sometimes is slow in getting the complete feed and that's why i get too many incomplete archives.</div><br><br>Do you know who is providing the usenet feed to your ISP? I was having real trouble with astraweb; things have improved with a giganews based provider.<br><br>(I have removed the request for 221000 in my previous post.)</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">reopert wrote:</div><div class=""quotecontent"">As i said, i can write a perl script for you that will do it, you will run it like:<br><br>./create-nfo.pl 233000<br><br>233000 being the folder with 1000 books. Though again I can only make one that will work on a unix-based system, if it's windows it may need some tweaking.<br><br>but as you said adding just a generic info file is already an excellent start.<br><br>Thank you.</div><br><br>I think the current minimal .nfo file is enough. My reasoning is basically ``security through obscurity''.  USENET is not completely invulnerable to takedown notices -- in the case of AV groups this has already become a problem. Having a multi-gigabyte archive filled with MD5-based filenames sans extensions is our first line of defence.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen151000esis–gen175000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:          MD5 Mismatches:<br>/151000        1000/1000        0<br>/152000        1000/1000        1<br>/153000        1000/1000        0<br>/154000        1000/1000        0<br>/155000        1000/1000        0<br>/156000        1000/1000        0<br>/157000        1000/1000        0<br>/158000        1000/1000        0<br>/159000        1000/1000        0<br>/160000        1000/1000        0<br>/161000        1000/1000        0<br>/162000        1000/1000        0<br>/163000        1000/1000        0<br>/164000        1000/1000        0<br>/165000        1000/1000        0<br>/166000        1000/1000        0<br>/167000        1000/1000        0<br>/168000        1000/1000        0<br>/169000        1000/1000        0<br>/170000        1000/1000        0<br>/171000        1000/1000        0<br>/172000        1000/1000        0<br>/172000        1000/1000        0<br>/174000        1000/1000        0**<br>/175000        1000/1000        0</div>** IMPORTANT: These numbers assume you have repaired corrupted RAR volumes using RAR Recovery Records. See the previous how-to post on using RAR recovery records.<br><br><br><span style=""font-weight: bold"">Corrupted Archives: Recoverable using RAR Recovery Records<br></span>After PAR repair gen174000esis was reported as being corrupt. gen174000esis.part123.rar was identified as containing a file with a CRC error:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Testing archive gen174000esis.part123.rar<br>174000/cb2c4c75f94c9f86ed7c7b8fd4b3c352 - CRC failed</div>gen174000esis.part123.rar  was restored using the built-in RAR recovery records. The gen174000esis archive was then decompressed without error.<br><br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found the following MD5 mismatches:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/152000/4f2ef3d05afbafd986c8aa8d7141a0b3 does NOT match<br></div><br><span style=""font-weight: bold"">Suggestions and Notes:<br></span>(1) Check the file with the MD5 mismatch and upload a patch for this file. <br>(2) The error rate on this batch is again way down. Excellent.<br>(3) This batch was a huge download: approximately 297.35GB, uncompressed.<br><br>As always my thanks to 123dutch and texas for their fine work.</div>",
,,,"<div class=""postbody"">Requests for reposts:<br>gen103000esis (ansaron's request)<br>gen106000esis (ansaron's request)<br>gen118000esis (ansaron's request)<br>gen119000esis (ansaron's request) <br>gen120000esis (ansaron's request)<br>gen121000esis (ansaron's request)<br>gen220000esis (reopert's request)<br><br>Have been reposted today.<br><br>NZB's will be added later.<br><br>Sorry for my absence. I was having health problems.<br>I will try to focus more on Genesis, though I am still not totally ok, but the worst is over.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">123dutch wrote:</div><div class=""quotecontent"">Requests for reposts:<br><br>Sorry for my absence. I was having health problems.<br>I will try to focus more on Genesis, though I am still not totally ok, but the worst is over.</div><br>Dear 123dutch,<br><br>I'm so sorry to hear of your health troubles. Please accept my good wishes for a speedy recovery -- chicken soup and bedrest recommended! <br><br>I've downloaded the re-uploaded archives 119000--121000 and these seem fine. I'll edit my previous report on that batch a little later.<br><br>I'm close to completing the download of the next batch of 25 archives, the report on which  will also be posted soon (my DSL line was hit by lightning and a sequence of power outages did not help the situation -- I'm lagging far behind in my download schedule.).   <br><br>With kind regards,<br>A.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen176000esis–gen200000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:          MD5 Mismatches:<br>/176000        1000/1000        0<br>/177000        1000/1000        0<br>/178000        1000/1000        0<br>/179000        1000/1000        0<br>/180000        1000/1000        0<br>/181000        1000/1000        0<br>/182000        1000/1000        0<br>/183000        1000/1000        0<br>/184000        1000/1000        0<br>/185000        1000/1000        0<br>/186000        1000/1000        0<br>/187000        1000/1000        0<br>/188000        1000/1000        0<br>/189000        1000/1000        0<br>/190000        1000/1000        0<br>/191000        1000/1000        0<br>/192000        1000/1000        0<br>/193000        1000/1000        0<br>/194000        1000/1000        0<br>/195000        1000/1000        0<br>/196000        1000/1000        0<br>/197000        1000/1000        0<br>/198000        1000/1000        0<br>/199000        1000/1000        0<br>/200000         996/996         0</div><br><span style=""font-weight: bold"">Corrupted Archives:</span><br>There were NO CORRUPTED ARCHIVES.<br><br><span style=""font-weight: bold"">MD5 Mismatches:</span><br>There were NO MD5 MISMATCHES.<br><br><span style=""font-weight: bold"">Notes:</span><br>(1) The total size of this batch, uncompressed, is approximately 391.8GB.<br>(2) Considering the size of the download the fact that there NO ERRORS AT ALL is impressive. Well done!</div>",
,,,"<div class=""postbody""><span style=""font-size: 150%; line-height: normal""><span style=""font-weight: bold"">Report on Libgen Usenet Postings gen201000esis–gen225000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:          MD5 Mismatches:<br>/201000         998/998         0<br>/202000         998/998         0<br>/203000         999/999         0<br>/204000        1000/1000        0<br>/205000        1000/1000        0<br>/206000        1000/1000        0<br>/207000        1000/1000        0<br>/208000        1000/1000        0<br>/209000        1000/1000        0<br>/210000         999/999         0<br>/211000        1000/1000        0<br>/212000         913/913         0<br>/213000         917/917         0<br>/214000        1000/1000        0<br>/215000        1000/1000        0<br>/216000        1000/1000        2<br>/217000        1000/1000        0<br>/218000        1000/1000        0<br>/219000        1000/1000        0<br>/220000        1000/1000        0<br>/221000        1000/1000        0<br>/222000        1000/1000        0<br>/223000         999/999         1<br>/224000         999/999         0<br>/225000         998/998         0</div><br><span style=""font-weight: bold"">Corrupted Archives:<br></span>There were NO corrupted archives.<br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found the following MD5 mismatches:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Archive:/216000<br>/216000/4c90a52005910a3b9dd4f384686d2b1f does NOT match<br>/216000/b3c50b8ed7a2e8de469feffe5a2a300f does NOT match<br>Archive:/223000<br>/223000/6f5d305df749522359cd4facae3d31d8 does NOT match<br></div></div>",
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,texas,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,Guest,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,texas,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,silverware,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,texas,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,reopert,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,123dutch,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,ansaron,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,bookwarrior,,bookwarrior
Library Genesis on Usenet,http://genofond.org/viewtopic.php?f=17&start=25&t=6769,texas,,bookwarrior
,,,"<div class=""postbody"">i guess you can just split it into two parts, why to keep the 1-to-1 identity with LG? there is no reason for this. I.e. you can make two files<br>gen121000esis-part1of2<br>gen121000esis-part2of2<br><br>that's pretty much it, if I understood the problem. No animals will be harmed.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">123dutch<br><br>somewhen after you filter out the suspecious infected files, we'd appreciate the listing: in most cases in the past it was false positive, but there are many executables in LG yet (man power problem). Although 1 in a 1000 looks quite wrong, especially if it's in the first 100k. But it's all possible of course.</div><br><br>Hi bw, we finished checking all the folders regarding the number of files in each one, and a virus-check on them.<br><br>We found only 3 virus detected by MS Essentials (all .exe files):<br>118000\fdc00245581b73b84d50cb4ef7a6143d<br>210000\61415ac7cfb96b7fcb967f5532b073e9<br>224000\140d912e60dd2fd88f75b9d7f2c6a653<br><br>In the past, I had some experiences with these ebook compilers that were identified as virus, though there were false positives, and maybe it's the case (better recheck with other antivirus as well).<br><br>About the MD5 checks (thanks ansaron and bw for the idea &amp; checks), we'e already on the folder 250.000, and in next days we hope to finish this as well. In the files already uploaded in Usenet, we found only 5 files with MD5 errors, which are:<br>35000\342ee41dd4c552bb8a14885b3064dbcb<br>35000\4f03f64700ca520a36ce3d7ea6356dfc<br>37000\e950c65a6e4e4df7e3a6d7a5cbe72e13<br>46000\6ef53368dd0404612fc4dbe932dbe21a<br>49000\211d062cd4a5bf38380bb413446df433<br><br>We are thinking if we reupload all these folders again, with the correct files fixed, or just these 5 files to be replaced (as a 'patch', so people download and overwrite our uploaded folders with them). Now we are double checking to avoid any MD5 or unrarring errors in future NZBs  <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""><br><br>Please keep coming with suggestions, ideas and checking the files, all this helped us to fix a lot of things too <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",
,,,"<div class=""postbody"">yes, that's how it's been happening with us: first upload of LG of about 50k took me 3 almost full attempts, making me iteratively learn what the issues were, just think of this number, it's satanic, if to have no mechanism of file integrity checking. If batches of folders arrive with no errors, it is more a pleasant exception, than a rule. But once you check them after writing onto an HDD, they are going to be more stable, than during the transfer process (including HDD writing). Reading isn't harmful, the rest of the file lifetime will only depend on the hardware lifetime which is the fundamental limit, from our statistics no longer that 1 year per HDD in average when files are accessed randomly, but I can't give a figure how often, possibly at maximum ~10 files/min in average.<br><br>btw, it's not only about HDDs of course, TCP/IP also admits some rate of errors, it does not absolutely guarantee delivery, it has checking mechanisms, but they do not guarantee all 100%.</div>",
,,,"<div class=""postbody"">Dear 123dutch,<br><br>I've downloaded the repost of gen22000esis.<br><br>First, the change to 100MB rar segments has helped: this is the first large (&gt;5GB) libgen archive that I've downloaded that did not require any par files. It also decompressed without error.<br><br>All MD5 hashes are verified too. Outstanding!<br><br>I'll edit my previous report post to point out that the repost of gen22000esis is error free. <br><br>Thanks,<br>A.</div>",
,,,"<div class=""postbody"">Hi 123dutch,<br><br><!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_nzb/gen107000esis.rar"">ftp://libgen.org/repository_nzb/gen107000esis.rar</a><!-- m --> is broken. If I search for the nzb elsewhere it says that it's incomplete: <!-- m --><a class=""postlink"" href=""http://www.nzbsearch.net/search.aspx?q=gen107000esis&amp;st=5"">http://www.nzbsearch.net/search.aspx?q= ... 0esis&amp;st=5</a><!-- m --><br><br>Can you please repost it? Thanks.<br><br>Thank you</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen26000esis–gen50000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br>/26000        1000        0 <br>/27000        1000        0<br>/28000        1000        0<br>/29000        1000        0<br>/30000        1000        0<br>/31000        1000        0<br>/32000        1000        0<br>/33000        1000        0<br>/34000        1000        0<br>/35000        1000        2 (0 with patch)<br>/36000        1000        0<br>/37000        1000        1 (0 with patch)<br>/38000        1000        0<br>/39000        1000        0<br>/40000        1000        0<br>/41000        1000        0<br>/42000        1000        0<br>/43000         999        0<br>/44000        1000        0      <br>/45000        1000        0<br>/46000        1000        1 (0 with patch)<br>/47000         999        0        <br>/48000        1000        0 <br>/49000        1000        1 (0 with patch)       <br>/50000        1000        0<br></div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>All archives except gen43000esis and gen47000esis expanded correctly.  <br><br>In gen43000esis the rar archive was reported as corrupt. 999/1000 files were extracted with the remaining file being reported as corrupt:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">!   F:\gen43000esis\gen43000esis.part3.rar: CRC failed in 43000\6e7c0c60951c7cc6a06ff67dfc6907ec. The file is corrupt<br></div><br>In gen47000esis the rar archive was reported as corrupt. 999/1000 files were extracted with the remaining file being reported as corrupt:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">!   F:\gen47000esis\gen47000esis.part4.rar: CRC failed in 47000\af253377df19e1631cc37ae2600ad74d. The file is corrupt<br></div><br>In both cases my OS X unrar utilities were unable to decompress the rar archives; WinRar was however able to extract all of the uncorrupted files.<br>All of the files that were extracted from these two archives were verified as having the correct MD5 hash.<br><br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>In addition md5deep found MD5 hash mismatches for the files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 does NOT match<br>/35000/8be66d2107a296a63c2f526df184f37d does NOT match<br>/37000/e950c65a6e4e4df7e3a6d7a5cbe72e13 does NOT match<br>/46000/6ef53368dd0404612fc4dbe932dbe21a does NOT match<br>/49000/211d062cd4a5bf38380bb413446df433 does NOT match<br></div><br>(texas and 123dutch already know about the problems with three of these five files. Their MD5 mismatch results for /35000 do not correlate with mine. Please check.)<br><br><br><span style=""font-weight: bold"">Suggestions: <br></span>(1) Create a `patch' file to replace the files with MD5 hash mismatches (re-uploading whole archives is overkill).<br>(2) Upload corrected versions of gen43000esis and gen47000esis since the rar archives are rejected entirely by many decompression utilities.<br><br><br>As always, thanks to 123dutch and texas for all their work in getting Library Genesis onto Usenet.<br><br><span style=""font-weight: bold"">Edit:</span> <br>(1) Remark on MD5 mismatches in /35000 files.<br>(2) (9 February 2013): gen PATCH esis corrects the mismatch errors in /35000, /37000, 46000, 49000. In the case of /35000 you should also delete the existing two files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 <br>/35000/8be66d2107a296a63c2f526df184f37d</div></div>",
,,,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent""><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>In addition md5deep found MD5 hash mismatches for the files:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">/35000/4e04cd97600452a602cae12f28ab43c1 does NOT match<br>/35000/8be66d2107a296a63c2f526df184f37d does NOT match<br>/37000/e950c65a6e4e4df7e3a6d7a5cbe72e13 does NOT match<br>/46000/6ef53368dd0404612fc4dbe932dbe21a does NOT match<br>/49000/211d062cd4a5bf38380bb413446df433 does NOT match<br></div><br>(texas and 123dutch already know about the problems with three of these five files. Their MD5 mismatch results for /35000 do not correlate with mine. Please check.)<br><br></div><br><br>Thanks again, ansaron, for your great work checking the files, about the corrupted RAR Archives, I believe dutch will reupload them soon. <br><br>Regarding the MD5, the best way indeed will be to upload just a patch with the 5 files, however, regarding the /35000 folder (with the 2 problematic files), I believe they were replaced in the next versions of the LG database. <br><br>For example, the file 4e04cd97600452a602cae12f28ab43c1 isn't found in libgen.org if we perform a MD5 search. But, if we hash the actual file, the MD5 will be 4f03f64700ca520a36ce3d7ea6356dfc (so there's a mismatch as you correctly observed), and now this file is in LG:<br><br><!-- m --><a class=""postlink"" href=""http://libgen.org/book/index.php?md5=4F03F64700CA520A36CE3D7EA6356DFC"">http://libgen.org/book/index.php?md5=4F ... 7EA6356DFC</a><!-- m --><br><br>The same with the 8be66d2107a296a63c2f526df184f37d, whose MD5 is 342ee41dd4c552bb8a14885b3064dbcb, and we find <!-- m --><a class=""postlink"" href=""http://libgen.org/book/index.php?md5=342ee41dd4c552bb8a14885b3064dbcb"">http://libgen.org/book/index.php?md5=34 ... 5b3064dbcb</a><!-- m --><br><br>So, I believe that when the torrent was started, these 2 files had errors and only after, the database was fixed, but the torrents kept the original files (please bw and Bill, correct me if I'm wrong). <br><br>The other 3 files indeed had erros in our folders. I uploaded the patch <a href=""http://www.filefactory.com/file/2m9mhyaptq0p/n/LG_Patch_rar"" class=""postlink"">here</a>, but soon dutch will put this rar in Usenet.<br><br>Thanks again, and please keep bringing these infos, and any suggestions.</div>",
,,,"<div class=""postbody"">texas, thank you very much for the reply, which is very reassuring. I was puzzled and quite concerned by this anomaly. I am happy though that the checks we have in place are picking up these problems –- the underlying LibGen filing system is well thought out and is doing its job.<br><br>The patch file seems perfect. There are now no MD5 mismatch errors in the corrected directories. Once the patch file is uploaded to Usenet I will edit the previous report, including a note that the two anomalous files in /35000 need to be deleted.<br><br>The error rate is very low. As bookwarrior pointed out we should expect some errors in the early stages. I think once we get past /104000 the combination of MD5 checks + RAR repair files + smaller RAR segments will force the error rate down to zero (I hope!). <br><br>Best wishes,<br>A.</div>",
,,,"<div class=""postbody"">I believe there must be some optimal length of the crc code for a particular error correction/recovery algorithm. Possibly RAR has some open numbers or some easy tests can be made with the archiver itself (pack with and without recovery option, compare the sizes) to figure out what fraction of the data the recovery code should optimally take. With a binary/hex editor one can modify 1, 2, ..., N bits to see how the algorithm reacts. The numbers on the reliability of HDD writing and TCP/IP can be found, this will give a firm figure (I someone is heroic enough to get thru) what the reasonable amount of recovery information should be per unit of volume. I'm just speculating...</div>",
,,,"<div class=""postbody"">Re: error rates - see this W#kipedia article <a href=""http://en.wikipedia.org/wiki/ZFS#Error_Rates_in_Harddisks"" class=""postlink"">http://en.wikipedia.org/wiki/ZFS#Error_Rates_in_Harddisks</a></div>",
,,,"<div class=""postbody"">there are several broken nzb files on <!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_nzb/"">ftp://libgen.org/repository_nzb/</a><!-- m --> nzb's 118-121 that is - most files come out broken. If I get the nzb from another source like <!-- m --><a class=""postlink"" href=""http://www.nzbsearch.net/search.aspx?q=gen121000esis&amp;st=5"">http://www.nzbsearch.net/search.aspx?q= ... 0esis&amp;st=5</a><!-- m --> it works fine. Thank you.</div>",
,,,"<div class=""postbody"">this tuesday the upload continues again: 43000, 47000, 107000 and contuing with 131000 and up.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen51000esis–gen75000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br>/51000        1000        0 <br>/52000        1000        0<br>/53000        1015?       0<br>/54000        1000        0<br>/55000        1001?       0<br>/56000        1000        0<br>/57000         999        0<br>/58000        1000        0<br>/59000        1000        0<br>/60000        1000        0<br>/61000        1000        0<br>/62000        1000        0<br>/63000        1000        0<br>/64000        1000        0<br>/65000        1000        0<br>/66000        1000        0<br>/67000        1000        0<br>/68000         999        0<br>/69000        1000        0     <br>/70000         997        0<br>/71000         996        0<br>/72000         998        0       <br>/73000         997        0<br>/74000        1000        0        <br>/75000         994        0<br></div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>NO CORRUPTED ARCHIVES. All archives expanded correctly.  <br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found NO MD5 MISMATCHES.<br><br><span style=""font-weight: bold"">File Numbers:<br></span>Could I have confirmation of the correct/expected number of files in each directory? <br><br><span style=""font-weight: bold"">Remark on Patches:<br></span>gen PATCH esis corrects previous MD5 mismatches in /35000, /37000/, /46000 and /49000. A note about this will be added to the previous report.<br><br>As always, thanks to 123dutch and texas for their work. Your efforts are deeply appreciated. The error rate for this batch was zero. Outstanding!</div>",
,,,"<div class=""postbody"">any reason why 81000 and 82000 are missing? they were skipped and went right into 83000</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">reopert wrote:</div><div class=""quotecontent"">any reason why 81000 and 82000 are missing? they were skipped and went right into 83000</div><br><br>They do not exist/are empty.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">ansaron wrote:</div><div class=""quotecontent""><br><span style=""font-weight: bold"">File Numbers:<br></span>Could I have confirmation of the correct/expected number of files in each directory? <br></div><br>Here is the list of the number of files in each directory, according to the torrent files. In our NZBs we just removed 3 files, detected as virus by MS Essentials (.exe files), so these 3 folders will have 999 files in the NZBs. The list is:<br><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">#Files  Folder<br>----   -----<br>54    /83000<br>257   /77000<br>290   /80000<br>318   /865000<br>351   /866000<br>515   /76000<br>913   /212000<br>917   /213000<br>985   /52000<br>994   /75000<br>996   /200000<br>996   /237000<br>996   /71000<br>997   /107000<br>997   /248000<br>997   /308000<br>997   /70000<br>997   /73000<br>997   /869000<br>998   /108000<br>998   /124000<br>998   /201000<br>998   /202000<br>998   /225000<br>998   /72000<br>998   /721000<br>998   /844000<br>999   /0<br>999   /104000<br>999   /114000<br>999   /115000<br>999   /118000 * virus (.exe - /118000/fdc00245581b73b84d50cb4ef7a6143d)<br>999   /203000<br>999   /210000 * virus (.exe - /210000/61415ac7cfb96b7fcb967f5532b073e9)<br>999   /223000<br>999   /224000 * virus (.exe - /224000/140d912e60dd2fd88f75b9d7f2c6a653)<br>999   /226000<br>999   /239000<br>999   /244000<br>999   /251000<br>999   /294000<br>999   /307000<br>999   /327000<br>999   /57000<br>999   /786000<br>999   /867000<br>1001   /55000<br>1015   /53000<br><br>All other folders = 1000 files</div><br><br>Some folders indeed have much less than 1000 files. Maybe BW or Bill can confirm or correct these numbers, if possible, so our NZBs can be up-to-date or eventually corrected to the actual LG now.<br><br>Thanks again for checking the NZBs! Your help and of course BW and Bill_G efforts are also outstanding, no doubt at all!  <img src=""./images/smilies/icon_e_biggrin.gif"" alt="":D"" title=""Very Happy""><br><br>Cheers!<br>Texas</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">Report on Libgen Usenet Postings gen76000esis–gen100000esis:<br></span></span><br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">Directory:   # Files:   MD5 Mismatches:<br> /76000          515        0 <br> /77000          257        0<br> /78000         1000        0<br> /79000         1000        0<br> /80000          290        0<br> /81000         DOES NOT EXIST<br> /82000         DOES NOT EXIST<br> /83000           54        0<br> /84000         1000        0<br> /85000         1000        0<br> /86000         1000        0<br> /87000         1000        0<br> /88000         1000        0<br> /89000         1000        0<br> /90000         1000        0<br> /91000         1000        0<br> /92000         1000        0<br> /93000         1000        0<br> /94000         1000        0     <br> /95000         1000        0<br> /96000         1000        0<br> /97000         1000        0       <br> /98000         1000        0<br> /99000         1000        0        <br>/100000         1000        0</div><br><br><span style=""font-weight: bold"">Corrupted RAR Archives:<br></span>There were NO CORRUPTED ARCHIVES. The gen78000esis archive <span style=""font-style: italic"">was</span> reported as being corrupt by MacPAR deluxe's built-in RAR extractor. However all 1000 files were successfully extracted by other RAR utilities (on both OS X and Windows). WinRAR confirms all files in the archive as being uncorrupted. All MD5 hashes for extracted files are OK. I have no explanation for this (beyond that most OS X decompression utilities seem more fragile than their Windows counterparts). So, if you have trouble expanding the gen78000esis RAR archives try a different decompression utility.    <br><br><span style=""font-weight: bold"">MD5 Mismatches:<br></span>md5deep found NO MD5 MISMATCHES.<br><br><br>The error rate for this batch was again zero. We are winning  <img src=""./images/smilies/icon_cool.gif"" alt=""8-)"" title=""Cool""></div>",
,,,"<div class=""postbody"">gen155000esis is missing at least one .rar file. It ends with part120, but it's incomplete - at least part 121 is missing. Getting only 992 files.</div>",
,,,"<div class=""postbody"">I will re-up gen155000esis monday <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""> Thanks for the reply <img src=""./images/smilies/icon_e_smile.gif"" alt="":)"" title=""Smile""></div>",
,,,"<div class=""postbody"">our gen.lib.rus.ec's hoster seems to have sold his project (lib.rus.ec) or is close to this. No one knows what the fate of gen.lib.rus.ec is. So, the backups of any sort become even more important now.</div>",
,,,"<div class=""postbody""><div class=""quotetitle"">bookwarrior wrote:</div><div class=""quotecontent"">our gen.lib.rus.ec's hoster seems to have sold his project (lib.rus.ec) or is close to this. No one knows what the fate of gen.lib.rus.ec is. So, the backups of any sort become even more important now.</div><br><br>LG on usenet will continue (I am sick for a few weeks, hence the somewhat slower uploads). <br>But no matter what, we will keep on pushing the files to Usenet.</div>",
,,,"<div class=""postbody"">@123dutch<br><br>Someone else pointed this out before, but it's only just bitten me: gen118000esis -- gen121000esis are all incomplete (no matter which  NZBs you use). That is: the uploads are all missing too many blocks for repair to be possible (in many cases the PAR files are damaged too). <br><br>Would it be possible for you to either upload additional PAR files or else maybe totally re-upload these four archives again? At the moment they are damaged beyond repair.<br><br>Many thanks (and I hope you get better soon!).<br><br><span style=""font-weight: bold"">UPDATE:</span> I'm now trying to use the recovery volumes in the RAR volumes to first (partially) repair the RAR archives and then use the PAR files to complete the recovery of the archives. I seem to have succeeded in doing this for gen118000esis. It's taking some time so my next report will probably be delayed.</div>",
,,,"<div class=""postbody""><span style=""font-weight: bold""><span style=""font-size: 150%; line-height: normal"">How to use RAR Recovery Records. Part 1<br></span></span><br>From gen105000esis onwards 123dutch has incorporated recovery records into the RAR archives he is posting on Usenet. This was new to me, so I thought I'd write a short How-To on using RAR recovery records. <br><br>I'll illustrate using two examples from 123dutch's Usenet upload of Library Genesis: gen126000esis and gen118000esis<br><br>We'll start with gen126000esis as this is more straightforward. After downloading the RAR volumes and repairing with PAR2 files, the standard decompression utilities reported that the archive was damaged. <br><br><span style=""font-weight: bold"">Windows</span><br>If you are using WinRAR on a Windows machine then open any of the gen12600esis RAR volumes (say gen126000esis.part001.rar) in WinRAR and select ""test"". <br>After a while WinRAR will identify one volume (gen126000esis.part005.rar) as containing a file with a CRC error.<br>Open gen126000esis.part005.rar with WinRAR and select ""repair"". <br>After a while WinRAR will produce a file: fixed.gen126000esis.part005.rar. <br>Delete the original file gen126000esis.part005.rar and rename fixed.gen126000esis.part005.rar to gen126000esis.part005.rar<br><br>gen126000esis should now decompress without any problems.<br><br><span style=""font-weight: bold"">*nix<br></span>If you are using a *nix variant (linux, OS X, ...) then you will need to use the command-line version of RAR (available from:<a href=""http://www.rarlab.com/download.htm"" class=""postlink"">http://www.rarlab.com/download.htm</a>).<br><br>We start by testing the RAR volumes to identify the volume(s) that requires repair. Change to the directory containing the rar executable and in a terminal window type:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">./rar t '/path/to/rar/volumes/*'</div><br>You should get output that reads:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">RAR 4.20   Copyright (c) 1993-2012 Alexander Roshal   9 Jun 2012<br>Trial version             Type RAR -? for help<br><br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part001.rar<br><br>Testing     126000/00528e2366972d6c027fb4a15e4b18cf                   OK <br>Testing     126000/0093fe6589d2a286206f9d181f9b6d4b                   OK <br>Testing     126000/00c1b48fda5f2f0072710c19a6f2a4c1                   OK <br>Testing     126000/00e61e1d50e9f106cca8e37d093fcaed                   OK<br>...<br>...<br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part005.rar<br><br>...         126000/0792160377d05abb306e2bac7cc43c5f                   OK <br>Testing     126000/079e0eb1cdfbc062e75758ccef635a61                    2%<br>126000/079e0eb1cdfbc062e75758ccef635a61 - CRC failed<br>Testing     126000/07d10822167b765b0031b5975b6a1ea2                   OK <br>Testing     126000/07dbc84267f12801736dbafcf791661f                   OK <br>Testing     126000/0807b545d7f25c85cc9dc0068ec816be                   OK <br>Testing     126000/08a244119244b8efe3c6f04b45ab5527                   OK <br>Testing     126000/08be1615b9af3c644a222012e775d9e0                   OK <br>Testing     126000/0910705a9d46846f5efdcac1e1d1b146                   OK <br>Testing     126000/092a34a90c230bc814dd3163398ca4e7                    2%<br>...<br>...<br><br>Testing archive /Users/ansaron/Downloads/usenet/gen126000esis/gen126000esis.part086.rar<br><br>...         126000/fb931f2d8b7c464ed78c6f1e51cca6a7                   OK <br>Testing     126000/fbb9b0426e09282efcb2164f04a253dc                   OK <br>Testing     126000/fbdbcbe36bb3e00e805ee3f9470be247                   OK <br>Testing     126000/fc2d16b77ef81815aaf9aee3a154ced8                   OK <br>...<br>...<br>Total errors: 1<br></div><br>So in this case there is a single RAR volume (part005) containing a corrupted file.  We now repair that RAR volume:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">./rar r /path/to/rar/volumes/gen126000esis.part005.rar</div><br>You should get output:<br><div class=""codetitle""><b>Code:</b></div><div class=""codecontent"">RAR 4.20   Copyright (c) 1993-2012 Alexander Roshal   9 Jun 2012<br>Trial version             Type RAR -? for help<br><br>Building fixed.gen126000esis.part005.rar<br>Scanning...<br>Data recovery record found 100%<br>Sector 36051 (offsets 119A600...119A800) damaged - data recovered<br>Sector 36052 (offsets 119A800...119AA00) damaged - data recovered<br>Sector 36053 (offsets 119AA00...119AC00) damaged - data recovered<br>Sector 36054 (offsets 119AC00...119AE00) damaged - data recovered<br>Sector 36055 (offsets 119AE00...119B000) damaged - data recovered<br>Done</div><br>and you end up with a file fixed.gen126000esis.part005.rar (in the same directory where the rar executable lives). Replace the original part005 volume with the fixed volume.  The  gen126000esis archive should now decompress without error.</div>",
,,,"<div class=""postbody"">thanks, I believe it will be helpful to others.</div>",
,,,"<div class=""postbody"">@ansaron: Thanks a lot for your tutorial, and for you testings as well!<br><br>@BW: Today and in next days were uploading the books from folders 200k-300k, and we noticed that there's the fllowing batch file which deletes some books with broken links (MD5 mismatches?):<br><!-- m --><a class=""postlink"" href=""ftp://libgen.org/repository_torrent/del%20broken%20files.bat"">ftp://libgen.org/repository_torrent/del ... 0files.bat</a><!-- m --><br><br>So, it's better to delete the books from these folders, by running the .bat, and after they are removed, upload in Usenet the folders without these books already?<br><br>Thanks you all again for your great work!</div>",
